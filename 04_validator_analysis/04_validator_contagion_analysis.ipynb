{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0b70dfd",
   "metadata": {},
   "source": [
    "# Validator Relationship Contagion Analysis\n",
    "\n",
    "Comprehensive framework for analyzing MEV vulnerability contagion through validator relationships.\n",
    "\n",
    "## Key Analysis Components:\n",
    "1. **Leader Slot Concentration as Attractor** - Identify validator hotspots\n",
    "2. **Specialized Exploitation through Validator-AMM Relationships** - Bot targeting patterns\n",
    "3. **Exploitation of Slot Boundary Delays** - Cross-slot attack patterns (2Fast bots)\n",
    "4. **Systematic Bot Targeting Ecosystem** - Attacker specialization and infrastructure advantages\n",
    "5. **Contagion Pathways** - How attacks spillover across protocols\n",
    "6. **Mitigation Effectiveness** - TWAP, commit-reveal, slot-level filtering recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92946ba",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123bf047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from collections import defaultdict, Counter\n",
    "from typing import Dict, List, Tuple, Set, Any\n",
    "from dataclasses import dataclass, asdict\n",
    "import json\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f012cb",
   "metadata": {},
   "source": [
    "## Data Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374bb33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ValidatorHotspot:\n",
    "    \"\"\"Represents a validator concentration hotspot.\"\"\"\n",
    "    validator_address: str\n",
    "    total_mev_count: int\n",
    "    unique_attackers: int\n",
    "    unique_protocols: int\n",
    "    concentration_ratio: float  # % of all MEV in dataset\n",
    "    avg_attacks_per_slot: float\n",
    "    slots_active: int\n",
    "    risk_level: str  # HIGH, MEDIUM, LOW\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ContagionPath:\n",
    "    \"\"\"Represents a contagion pathway between validator-protocol pairs.\"\"\"\n",
    "    source_validator: str\n",
    "    source_protocol: str\n",
    "    target_validators: List[str]\n",
    "    target_protocols: List[str]\n",
    "    shared_attackers: List[str]\n",
    "    contagion_strength: float  # 0-1, based on shared execution\n",
    "    temporal_correlation: float  # How attacks cluster in time\n",
    "    spillover_evidence: int  # Number of cross-slot jumps observed\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BotSpecialization:\n",
    "    \"\"\"Represents a bot's specialization pattern.\"\"\"\n",
    "    bot_address: str\n",
    "    preferred_validators: Dict[str, int]  # validator -> count\n",
    "    preferred_protocols: Dict[str, int]  # protocol -> count\n",
    "    attack_types: Dict[str, int]  # sandwich, front_run, etc\n",
    "    infrastructure_score: float  # Based on timing precision\n",
    "    success_rate: float\n",
    "    avg_profit_per_attack: float\n",
    "\n",
    "print(\"✓ Data models defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a11249",
   "metadata": {},
   "source": [
    "## ValidatorContagionAnalyzer Class\n",
    "\n",
    "Main analyzer class with methods for each analysis component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b3e50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidatorContagionAnalyzer:\n",
    "    \"\"\"\n",
    "    Comprehensive validator relationship and MEV contagion analyzer.\n",
    "    \n",
    "    Identifies how validator relationships create systemic vulnerabilities\n",
    "    that propagate across protocols and time periods.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir: str = \"02_mev_detection\", \n",
    "                 parallel_workers: int = 4):\n",
    "        \"\"\"\n",
    "        Initialize the contagion analyzer.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data_dir : str\n",
    "            Directory containing MEV detection results\n",
    "        parallel_workers : int\n",
    "            Number of parallel workers for analysis\n",
    "        \"\"\"\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.mev_data = None\n",
    "        self.validator_graph = None\n",
    "        self.contagion_paths = []\n",
    "        self.bot_specializations = {}\n",
    "        self.hotspots = {}\n",
    "        self.cross_slot_patterns = []\n",
    "        self.parallel_workers = parallel_workers\n",
    "        \n",
    "    def load_mev_data(self, filepath: str = None):\n",
    "        \"\"\"Load MEV detection results.\"\"\"\n",
    "        if filepath is None:\n",
    "            filepath = str(self.data_dir / \"per_pamm_all_mev_with_validator.csv\")\n",
    "        \n",
    "        self.mev_data = pd.read_csv(filepath)\n",
    "        print(f\"✓ Loaded {len(self.mev_data):,} MEV records\")\n",
    "        print(f\"  Validators: {self.mev_data['validator'].nunique()}\")\n",
    "        print(f\"  Attackers: {self.mev_data['attacker_signer'].nunique()}\")\n",
    "        print(f\"  Protocols: {self.mev_data['amm_trade'].nunique()}\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "print(\"✓ Analyzer class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da82d666",
   "metadata": {},
   "source": [
    "## Part 1: Validator Hotspot Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461e0c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this method to ValidatorContagionAnalyzer class\n",
    "\n",
    "def identify_validator_hotspots(self, top_n: int = 20,\n",
    "                                concentration_threshold: float = 0.01) -> Dict[str, ValidatorHotspot]:\n",
    "    \"\"\"\n",
    "    Identify validator hotspots - validators with disproportionate MEV concentration.\n",
    "    \n",
    "    Addresses: \"Leader Slot Concentration as an Attractor\"\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    top_n : int\n",
    "        Number of top validators to analyze\n",
    "    concentration_threshold : float\n",
    "        Minimum % of total MEV for HIGH risk classification\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Dict[str, ValidatorHotspot]\n",
    "        Information about validator concentration hotspots\n",
    "    \"\"\"\n",
    "    if self.mev_data is None:\n",
    "        raise ValueError(\"Load MEV data first with load_mev_data()\")\n",
    "    \n",
    "    total_mev = len(self.mev_data)\n",
    "    \n",
    "    # Calculate validator statistics\n",
    "    validator_stats = {}\n",
    "    \n",
    "    for validator in self.mev_data['validator'].unique():\n",
    "        val_data = self.mev_data[self.mev_data['validator'] == validator]\n",
    "        \n",
    "        mev_count = len(val_data)\n",
    "        concentration = mev_count / total_mev\n",
    "        unique_attackers = val_data['attacker_signer'].nunique()\n",
    "        unique_protocols = val_data['amm_trade'].nunique()\n",
    "        \n",
    "        # Estimate slots active (based on unique slot/time combinations if available)\n",
    "        if 'slot' in val_data.columns:\n",
    "            slots_active = val_data['slot'].nunique()\n",
    "        else:\n",
    "            slots_active = len(val_data) // max(1, unique_attackers)  # Estimate\n",
    "        \n",
    "        avg_attacks_per_slot = mev_count / max(1, slots_active)\n",
    "        \n",
    "        # Risk classification\n",
    "        if concentration >= concentration_threshold:\n",
    "            risk_level = \"HIGH\"\n",
    "        elif concentration >= concentration_threshold * 0.5:\n",
    "            risk_level = \"MEDIUM\"\n",
    "        else:\n",
    "            risk_level = \"LOW\"\n",
    "        \n",
    "        hotspot = ValidatorHotspot(\n",
    "            validator_address=validator,\n",
    "            total_mev_count=mev_count,\n",
    "            unique_attackers=unique_attackers,\n",
    "            unique_protocols=unique_protocols,\n",
    "            concentration_ratio=concentration,\n",
    "            avg_attacks_per_slot=avg_attacks_per_slot,\n",
    "            slots_active=slots_active,\n",
    "            risk_level=risk_level\n",
    "        )\n",
    "        \n",
    "        validator_stats[validator] = hotspot\n",
    "    \n",
    "    # Sort by MEV count and return top N\n",
    "    sorted_validators = sorted(\n",
    "        validator_stats.items(),\n",
    "        key=lambda x: x[1].total_mev_count,\n",
    "        reverse=True\n",
    "    )[:top_n]\n",
    "    \n",
    "    self.hotspots = dict(sorted_validators)\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"VALIDATOR HOTSPOT ANALYSIS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\nTop {len(sorted_validators)} Validators by MEV Concentration:\\n\")\n",
    "    \n",
    "    for i, (validator, hotspot) in enumerate(sorted_validators, 1):\n",
    "        print(f\"{i:2d}. {validator[:16]}...\")\n",
    "        print(f\"    MEV Count: {hotspot.total_mev_count:,} ({hotspot.concentration_ratio*100:.2f}%)\")\n",
    "        print(f\"    Attackers: {hotspot.unique_attackers} | Protocols: {hotspot.unique_protocols}\")\n",
    "        print(f\"    Avg Attacks/Slot: {hotspot.avg_attacks_per_slot:.2f} | Risk: {hotspot.risk_level}\")\n",
    "        print()\n",
    "    \n",
    "    return self.hotspots\n",
    "\n",
    "# Attach to class\n",
    "ValidatorContagionAnalyzer.identify_validator_hotspots = identify_validator_hotspots\n",
    "print(\"✓ Validator hotspot identification method added\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95945f55",
   "metadata": {},
   "source": [
    "## Part 2: Validator-AMM Relationship Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad274535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_validator_amm_contagion(self, min_shared_attacks: int = 2) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze contagion through validator-AMM relationships.\n",
    "    \n",
    "    Addresses: \"Specialized Exploitation through Validator-AMM Relationships\"\n",
    "    \"\"\"\n",
    "    if self.mev_data is None:\n",
    "        raise ValueError(\"Load MEV data first\")\n",
    "    \n",
    "    results = {\n",
    "        'validator_protocol_pairs': [],\n",
    "        'high_risk_combinations': [],\n",
    "        'contagion_pathways': [],\n",
    "        'protocol_vulnerability_clusters': [],\n",
    "        'attacker_specialization': {}\n",
    "    }\n",
    "    \n",
    "    # 1. Build validator-protocol matrix\n",
    "    validator_protocol_matrix = defaultdict(lambda: defaultdict(int))\n",
    "    validator_protocol_attackers = defaultdict(lambda: defaultdict(set))\n",
    "    \n",
    "    for _, row in self.mev_data.iterrows():\n",
    "        validator = row['validator']\n",
    "        protocol = row['amm_trade']\n",
    "        attacker = row['attacker_signer']\n",
    "        \n",
    "        validator_protocol_matrix[validator][protocol] += 1\n",
    "        validator_protocol_attackers[validator][protocol].add(attacker)\n",
    "    \n",
    "    # 2. Identify high-risk validator-protocol combinations\n",
    "    all_pairs = []\n",
    "    for validator, protocols in validator_protocol_matrix.items():\n",
    "        for protocol, count in protocols.items():\n",
    "            unique_attackers = len(validator_protocol_attackers[validator][protocol])\n",
    "            pair_risk = count * unique_attackers  # Risk score = volume * diversity\n",
    "            \n",
    "            all_pairs.append({\n",
    "                'validator': validator,\n",
    "                'protocol': protocol,\n",
    "                'attack_count': count,\n",
    "                'unique_attackers': unique_attackers,\n",
    "                'risk_score': pair_risk,\n",
    "                'attackers': list(validator_protocol_attackers[validator][protocol])\n",
    "            })\n",
    "    \n",
    "    # Sort by risk score\n",
    "    all_pairs.sort(key=lambda x: x['risk_score'], reverse=True)\n",
    "    \n",
    "    results['validator_protocol_pairs'] = all_pairs[:50]\n",
    "    results['high_risk_combinations'] = [p for p in all_pairs if p['attack_count'] >= min_shared_attacks][:20]\n",
    "    \n",
    "    # 3. Detect contagion pathways\n",
    "    contagion_pathways = self._detect_validator_level_contagion(validator_protocol_attackers)\n",
    "    results['contagion_pathways'] = contagion_pathways\n",
    "    \n",
    "    # 4. Identify protocol vulnerability clusters\n",
    "    protocol_clusters = self._identify_protocol_vulnerability_clusters(validator_protocol_attackers, all_pairs)\n",
    "    results['protocol_vulnerability_clusters'] = protocol_clusters\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"VALIDATOR-AMM CONTAGION ANALYSIS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\nHigh-Risk Validator-Protocol Combinations:\\n\")\n",
    "    \n",
    "    for pair in results['high_risk_combinations'][:10]:\n",
    "        print(f\"  {pair['validator'][:16]}... + {pair['protocol']}\")\n",
    "        print(f\"    Attacks: {pair['attack_count']} | Unique Bots: {pair['unique_attackers']} | Risk: {pair['risk_score']}\")\n",
    "        print()\n",
    "    \n",
    "    print(f\"\\nDetected {len(results['contagion_pathways'])} Contagion Pathways\")\n",
    "    print(f\"Identified {len(results['protocol_vulnerability_clusters'])} Vulnerability Clusters\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def _detect_validator_level_contagion(self, validator_protocol_attackers: Dict) -> List[dict]:\n",
    "    \"\"\"Detect contagion pathways where same attackers exploit multiple protocols.\"\"\"\n",
    "    contagion_paths = []\n",
    "    \n",
    "    for validator, protocol_dict in validator_protocol_attackers.items():\n",
    "        for source_protocol, attackers in protocol_dict.items():\n",
    "            for target_protocol, target_attackers in protocol_dict.items():\n",
    "                if source_protocol != target_protocol:\n",
    "                    shared_attackers = attackers & target_attackers\n",
    "                    \n",
    "                    if len(shared_attackers) > 0:\n",
    "                        strength = len(shared_attackers) / len(attackers)\n",
    "                        \n",
    "                        contagion_paths.append({\n",
    "                            'validator': validator,\n",
    "                            'source_protocol': source_protocol,\n",
    "                            'target_protocol': target_protocol,\n",
    "                            'shared_attackers': list(shared_attackers),\n",
    "                            'num_shared': len(shared_attackers),\n",
    "                            'contagion_strength': strength\n",
    "                        })\n",
    "    \n",
    "    return sorted(contagion_paths, key=lambda x: x['num_shared'], reverse=True)\n",
    "\n",
    "def _identify_protocol_vulnerability_clusters(self, validator_protocol_attackers: Dict, all_pairs: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Identify clusters of protocols vulnerable through same validators.\"\"\"\n",
    "    clusters = []\n",
    "    \n",
    "    protocol_vulnerability = {}\n",
    "    for pair in all_pairs:\n",
    "        protocol = pair['protocol']\n",
    "        if protocol not in protocol_vulnerability:\n",
    "            protocol_vulnerability[protocol] = {'count': 0, 'validators': []}\n",
    "        protocol_vulnerability[protocol]['count'] += pair['attack_count']\n",
    "        protocol_vulnerability[protocol]['validators'].append(pair['validator'])\n",
    "    \n",
    "    protocols = list(protocol_vulnerability.keys())\n",
    "    for i, prot1 in enumerate(protocols):\n",
    "        for prot2 in protocols[i+1:]:\n",
    "            common_validators = set(protocol_vulnerability[prot1]['validators']) & \\\n",
    "                               set(protocol_vulnerability[prot2]['validators'])\n",
    "            \n",
    "            if len(common_validators) > 0:\n",
    "                clusters.append({\n",
    "                    'protocol_pair': [prot1, prot2],\n",
    "                    'shared_validators': list(common_validators),\n",
    "                    'num_shared': len(common_validators),\n",
    "                    'combined_risk': protocol_vulnerability[prot1]['count'] + protocol_vulnerability[prot2]['count']\n",
    "                })\n",
    "    \n",
    "    return sorted(clusters, key=lambda x: x['combined_risk'], reverse=True)\n",
    "\n",
    "ValidatorContagionAnalyzer.analyze_validator_amm_contagion = analyze_validator_amm_contagion\n",
    "ValidatorContagionAnalyzer._detect_validator_level_contagion = _detect_validator_level_contagion\n",
    "ValidatorContagionAnalyzer._identify_protocol_vulnerability_clusters = _identify_protocol_vulnerability_clusters\n",
    "print(\"✓ Validator-AMM contagion analysis methods added\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64172ba3",
   "metadata": {},
   "source": [
    "## Part 3: Cross-Slot Pattern Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0ce111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_cross_slot_patterns(self, slot_duration_ms: int = 400,\n",
    "                               time_column: str = 'ms_time') -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Detect exploitation of slot boundary delays - the 2Fast Bot pattern.\n",
    "    \n",
    "    Identifies multi-slot attack patterns where same attacker exploits\n",
    "    timing across slot boundaries to maximize MEV.\n",
    "    \"\"\"\n",
    "    if self.mev_data is None:\n",
    "        raise ValueError(\"Load MEV data first\")\n",
    "    \n",
    "    if 'slot' not in self.mev_data.columns or time_column not in self.mev_data.columns:\n",
    "        print(\"⚠ Warning: 'slot' or time column not available for cross-slot analysis\")\n",
    "        return {'status': 'unavailable', 'reason': 'Missing slot or time columns'}\n",
    "    \n",
    "    results = {\n",
    "        'multi_slot_attackers': [],\n",
    "        'cross_slot_sandwiches': [],\n",
    "        'slot_boundary_exploits': [],\n",
    "        'temporal_attack_clusters': []\n",
    "    }\n",
    "    \n",
    "    # 1. Identify attackers with trades spanning multiple slots\n",
    "    cross_slot_trades = []\n",
    "    \n",
    "    for attacker in self.mev_data['attacker_signer'].unique():\n",
    "        attacker_data = self.mev_data[self.mev_data['attacker_signer'] == attacker].copy()\n",
    "        \n",
    "        if len(attacker_data) < 3:\n",
    "            continue\n",
    "        \n",
    "        unique_slots = attacker_data['slot'].nunique()\n",
    "        if unique_slots > 1:\n",
    "            slot_list = sorted(attacker_data['slot'].unique())\n",
    "            \n",
    "            for i in range(len(slot_list) - 1):\n",
    "                slot_gap = slot_list[i+1] - slot_list[i]\n",
    "                if slot_gap <= 3:\n",
    "                    cross_slot_trades.append({\n",
    "                        'attacker': attacker,\n",
    "                        'slot_sequence': [slot_list[i], slot_list[i+1]],\n",
    "                        'slot_gap': slot_gap,\n",
    "                        'attack_type': 'potential_2fast_bot'\n",
    "                    })\n",
    "    \n",
    "    results['multi_slot_attackers'] = cross_slot_trades[:50]\n",
    "    \n",
    "    # 2. Identify cross-slot sandwiches\n",
    "    if 'fat_sandwich' in self.mev_data.columns:\n",
    "        fat_sandwich_data = self.mev_data[self.mev_data['fat_sandwich'] == True]\n",
    "        \n",
    "        for attacker in fat_sandwich_data['attacker_signer'].unique():\n",
    "            attacker_fat = fat_sandwich_data[fat_sandwich_data['attacker_signer'] == attacker]\n",
    "            slots = sorted(attacker_fat['slot'].unique())\n",
    "            \n",
    "            if len(slots) > 1:\n",
    "                results['cross_slot_sandwiches'].append({\n",
    "                    'attacker': attacker,\n",
    "                    'slots_involved': slots,\n",
    "                    'fat_sandwich_count': len(attacker_fat),\n",
    "                    'protocols': list(attacker_fat['amm_trade'].unique())\n",
    "                })\n",
    "    \n",
    "    # 3. Detect slot boundary timing exploits\n",
    "    if time_column in self.mev_data.columns:\n",
    "        boundary_exploits = []\n",
    "        \n",
    "        for slot in self.mev_data['slot'].unique():\n",
    "            slot_data = self.mev_data[self.mev_data['slot'] == slot].copy()\n",
    "            \n",
    "            if len(slot_data) > 0:\n",
    "                min_time = slot_data[time_column].min()\n",
    "                slot_data['time_in_slot'] = slot_data[time_column] - min_time\n",
    "                \n",
    "                boundary_trades = slot_data[slot_data['time_in_slot'] > slot_duration_ms * 0.9]\n",
    "                \n",
    "                if len(boundary_trades) > 0:\n",
    "                    boundary_exploits.append({\n",
    "                        'slot': slot,\n",
    "                        'boundary_trades': len(boundary_trades),\n",
    "                        'attackers': boundary_trades['attacker_signer'].nunique(),\n",
    "                        'protocols': boundary_trades['amm_trade'].nunique()\n",
    "                    })\n",
    "        \n",
    "        results['slot_boundary_exploits'] = sorted(boundary_exploits, key=lambda x: x['boundary_trades'], reverse=True)[:20]\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"CROSS-SLOT PATTERN DETECTION (2Fast Bot Analysis)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\nMulti-Slot Attacker Patterns: {len(results['multi_slot_attackers'])}\")\n",
    "    print(f\"Cross-Slot Fat Sandwiches: {len(results['cross_slot_sandwiches'])}\")\n",
    "    print(f\"Slot Boundary Exploits: {len(results['slot_boundary_exploits'])}\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "ValidatorContagionAnalyzer.detect_cross_slot_patterns = detect_cross_slot_patterns\n",
    "print(\"✓ Cross-slot pattern detection method added\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b22a437",
   "metadata": {},
   "source": [
    "## Part 4: Bot Ecosystem Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5a8a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_bot_ecosystem(self, top_n_bots: int = 50) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Map the systematic bot ecosystem.\n",
    "    \n",
    "    Identifies bot specialization, infrastructure advantages, and\n",
    "    competitive positioning in the Solana MEV landscape.\n",
    "    \"\"\"\n",
    "    if self.mev_data is None:\n",
    "        raise ValueError(\"Load MEV data first\")\n",
    "    \n",
    "    results = {\n",
    "        'bot_count': 0,\n",
    "        'top_bots': [],\n",
    "        'bot_specialization_matrix': [],\n",
    "        'validator_targeting': [],\n",
    "        'infrastructure_indicators': {},\n",
    "        'ecosystem_summary': {}\n",
    "    }\n",
    "    \n",
    "    # 1. Basic bot statistics\n",
    "    bot_stats = {}\n",
    "    \n",
    "    for bot in self.mev_data['attacker_signer'].unique():\n",
    "        bot_data = self.mev_data[self.mev_data['attacker_signer'] == bot]\n",
    "        \n",
    "        attack_count = len(bot_data)\n",
    "        unique_validators = bot_data['validator'].nunique()\n",
    "        unique_protocols = bot_data['amm_trade'].nunique()\n",
    "        \n",
    "        # Attack type distribution\n",
    "        attack_types = {}\n",
    "        for col in ['sandwich', 'front_running', 'back_running', 'fat_sandwich']:\n",
    "            if col in bot_data.columns:\n",
    "                attack_types[col] = int(bot_data[col].sum())\n",
    "        \n",
    "        # Success rate\n",
    "        if 'confidence' in bot_data.columns:\n",
    "            try:\n",
    "                confidence_numeric = pd.to_numeric(bot_data['confidence'], errors='coerce')\n",
    "                success_rate = confidence_numeric.mean()\n",
    "                if pd.isna(success_rate):\n",
    "                    success_rate = 0.5\n",
    "            except:\n",
    "                success_rate = 0.5\n",
    "        else:\n",
    "            success_rate = 0.5\n",
    "        \n",
    "        # Profitability\n",
    "        if 'net_profit_sol' in bot_data.columns:\n",
    "            total_profit = bot_data['net_profit_sol'].sum()\n",
    "            avg_profit = bot_data['net_profit_sol'].mean()\n",
    "        else:\n",
    "            total_profit = 0\n",
    "            avg_profit = 0\n",
    "        \n",
    "        # Timing precision\n",
    "        timing_precision = self._calculate_timing_precision(bot_data)\n",
    "        \n",
    "        bot_stats[bot] = {\n",
    "            'attack_count': attack_count,\n",
    "            'unique_validators': unique_validators,\n",
    "            'unique_protocols': unique_protocols,\n",
    "            'attack_types': attack_types,\n",
    "            'success_rate': success_rate,\n",
    "            'total_profit_sol': total_profit,\n",
    "            'avg_profit_sol': avg_profit,\n",
    "            'timing_precision': timing_precision,\n",
    "            'infrastructure_score': self._calculate_infrastructure_score(timing_precision, success_rate, unique_validators),\n",
    "            'preferred_validators': dict(bot_data['validator'].value_counts().head(5)),\n",
    "            'preferred_protocols': dict(bot_data['amm_trade'].value_counts().head(5))\n",
    "        }\n",
    "    \n",
    "    results['bot_count'] = len(bot_stats)\n",
    "    \n",
    "    # 2. Get top bots\n",
    "    top_bots_list = sorted(bot_stats.items(), key=lambda x: x[1]['attack_count'], reverse=True)[:top_n_bots]\n",
    "    \n",
    "    results['top_bots'] = [\n",
    "        {\n",
    "            'bot': bot,\n",
    "            'attack_count': stats['attack_count'],\n",
    "            'validators': stats['unique_validators'],\n",
    "            'protocols': stats['unique_protocols'],\n",
    "            'success_rate': stats['success_rate'],\n",
    "            'infrastructure_score': stats['infrastructure_score'],\n",
    "            'total_profit_sol': stats['total_profit_sol']\n",
    "        }\n",
    "        for bot, stats in top_bots_list\n",
    "    ]\n",
    "    \n",
    "    # 3. Infrastructure metrics\n",
    "    precision_scores = [stats['timing_precision'] for stats in bot_stats.values()]\n",
    "    infrastructure_scores = [stats['infrastructure_score'] for stats in bot_stats.values()]\n",
    "    \n",
    "    results['infrastructure_indicators'] = {\n",
    "        'mean_timing_precision_ms': np.mean(precision_scores),\n",
    "        'mean_infrastructure_score': np.mean(infrastructure_scores),\n",
    "        'high_quality_bots': sum(1 for s in infrastructure_scores if s > 0.7)\n",
    "    }\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"BOT ECOSYSTEM MAPPING\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\nTotal Unique Bots: {results['bot_count']}\")\n",
    "    print(f\"High-Infrastructure Bots (score > 0.7): {results['infrastructure_indicators']['high_quality_bots']}\")\n",
    "    print(f\"Mean Timing Precision: {results['infrastructure_indicators']['mean_timing_precision_ms']:.2f}ms\")\n",
    "    print(f\"\\nTop 10 Bots by Activity:\\n\")\n",
    "    \n",
    "    for i, bot_info in enumerate(results['top_bots'][:10], 1):\n",
    "        print(f\"{i:2d}. {bot_info['bot'][:16]}...\")\n",
    "        print(f\"    Attacks: {bot_info['attack_count']} | Validators: {bot_info['validators']} | Protocols: {bot_info['protocols']}\")\n",
    "        print(f\"    Success Rate: {bot_info['success_rate']:.2%} | Infrastructure: {bot_info['infrastructure_score']:.2f}\")\n",
    "        print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "def _calculate_timing_precision(self, bot_data: pd.DataFrame) -> float:\n",
    "    \"\"\"Calculate timing precision as indicator of infrastructure quality.\"\"\"\n",
    "    if 'time_diff_ms' in bot_data.columns:\n",
    "        timing_data = bot_data['time_diff_ms'].dropna()\n",
    "        if len(timing_data) > 0:\n",
    "            precision = 1 / (1 + timing_data.std() / 10)\n",
    "            return min(precision * 100, 100)\n",
    "    return 50\n",
    "\n",
    "def _calculate_infrastructure_score(self, timing_precision: float, success_rate: float, validator_diversity: int) -> float:\n",
    "    \"\"\"Calculate bot infrastructure quality score (0-10 scale).\"\"\"\n",
    "    timing_component = (timing_precision / 100) * 3\n",
    "    success_component = success_rate * 4\n",
    "    diversity_component = min(validator_diversity / 50, 1) * 3\n",
    "    return timing_component + success_component + diversity_component\n",
    "\n",
    "ValidatorContagionAnalyzer.map_bot_ecosystem = map_bot_ecosystem\n",
    "ValidatorContagionAnalyzer._calculate_timing_precision = _calculate_timing_precision\n",
    "ValidatorContagionAnalyzer._calculate_infrastructure_score = _calculate_infrastructure_score\n",
    "print(\"✓ Bot ecosystem mapping methods added\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c93864",
   "metadata": {},
   "source": [
    "## Part 5: Mitigation Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f28499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mitigation_recommendations(self) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate mitigation recommendations based on the analysis.\n",
    "    \n",
    "    Implements recommendations for:\n",
    "    - Slot-level MEV filtering\n",
    "    - TWAP (Time-Weighted Average Price) implementation\n",
    "    - Commit-reveal schemes\n",
    "    - Validator diversity requirements\n",
    "    \"\"\"\n",
    "    if self.mev_data is None:\n",
    "        raise ValueError(\"Load MEV data first\")\n",
    "    \n",
    "    recommendations = {\n",
    "        'slot_level_filtering': self._recommend_slot_filtering(),\n",
    "        'twap_implementation': self._recommend_twap(),\n",
    "        'commit_reveal_scheme': self._recommend_commit_reveal(),\n",
    "        'validator_diversity': self._recommend_validator_diversity(),\n",
    "        'bot_detection_rules': self._generate_bot_detection_rules(),\n",
    "        'implementation_priority': [\n",
    "            {\n",
    "                'rank': 1,\n",
    "                'strategy': 'Slot-Level MEV Filtering',\n",
    "                'impact': 'HIGH',\n",
    "                'effort': 'MEDIUM',\n",
    "                'estimated_reduction': '60-70% of coordinated attacks'\n",
    "            },\n",
    "            {\n",
    "                'rank': 2,\n",
    "                'strategy': 'TWAP-Based Oracle Updates',\n",
    "                'impact': 'HIGH',\n",
    "                'effort': 'MEDIUM',\n",
    "                'estimated_reduction': '50-60% of oracle-timed attacks'\n",
    "            },\n",
    "            {\n",
    "                'rank': 3,\n",
    "                'strategy': 'Commit-Reveal Transactions',\n",
    "                'impact': 'MEDIUM',\n",
    "                'effort': 'HIGH',\n",
    "                'estimated_reduction': '80-90% of sandwich attacks'\n",
    "            },\n",
    "            {\n",
    "                'rank': 4,\n",
    "                'strategy': 'Validator Diversity Enforcement',\n",
    "                'impact': 'MEDIUM',\n",
    "                'effort': 'LOW',\n",
    "                'estimated_reduction': '20-30% of concentrated attacks'\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"MITIGATION RECOMMENDATIONS\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    for item in recommendations['implementation_priority']:\n",
    "        print(f\"{item['rank']}. {item['strategy']}\")\n",
    "        print(f\"   Impact: {item['impact']} | Effort: {item['effort']}\")\n",
    "        print(f\"   Estimated Reduction: {item['estimated_reduction']}\")\n",
    "        print()\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "def _recommend_slot_filtering(self) -> Dict[str, Any]:\n",
    "    return {\n",
    "        'strategy': 'Implement slot-level MEV filtering at validator level',\n",
    "        'targets': ['High-concentration validators', 'BisonFi and HumidiFi protocols'],\n",
    "        'complexity': 'MEDIUM'\n",
    "    }\n",
    "\n",
    "def _recommend_twap(self) -> Dict[str, Any]:\n",
    "    return {\n",
    "        'strategy': 'Implement Time-Weighted Average Price (TWAP) oracle updates',\n",
    "        'complexity': 'MEDIUM',\n",
    "        'expected_impact': '50-60% reduction in back-run attacks'\n",
    "    }\n",
    "\n",
    "def _recommend_commit_reveal(self) -> Dict[str, Any]:\n",
    "    return {\n",
    "        'strategy': 'Implement commit-reveal transaction scheme',\n",
    "        'complexity': 'HIGH',\n",
    "        'user_experience_impact': 'Requires additional transaction + increased latency'\n",
    "    }\n",
    "\n",
    "def _recommend_validator_diversity(self) -> Dict[str, Any]:\n",
    "    return {\n",
    "        'strategy': 'Enforce validator diversity for critical operations',\n",
    "        'complexity': 'LOW',\n",
    "        'expected_impact': '20-30% reduction in concentrated attacks'\n",
    "    }\n",
    "\n",
    "def _generate_bot_detection_rules(self) -> Dict[str, Any]:\n",
    "    return {\n",
    "        'high_infrastructure_bot_detection': {\n",
    "            'indicators': ['Timing precision < 5ms', 'Success rate > 80%', 'Infrastructure score > 7.0'],\n",
    "            'action': 'Flag for validator MEV filtering'\n",
    "        },\n",
    "        'specialized_attack_pattern': {\n",
    "            'indicators': ['Attacks concentrated on 1-2 protocols (>70%)', 'Concentrated on 1-3 validators'],\n",
    "            'action': 'Recommend protocol-level defenses'\n",
    "        }\n",
    "    }\n",
    "\n",
    "ValidatorContagionAnalyzer.generate_mitigation_recommendations = generate_mitigation_recommendations\n",
    "ValidatorContagionAnalyzer._recommend_slot_filtering = _recommend_slot_filtering\n",
    "ValidatorContagionAnalyzer._recommend_twap = _recommend_twap\n",
    "ValidatorContagionAnalyzer._recommend_commit_reveal = _recommend_commit_reveal\n",
    "ValidatorContagionAnalyzer._recommend_validator_diversity = _recommend_validator_diversity\n",
    "ValidatorContagionAnalyzer._generate_bot_detection_rules = _generate_bot_detection_rules\n",
    "print(\"✓ Mitigation recommendation methods added\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d039bb68",
   "metadata": {},
   "source": [
    "def export_contagion_graph(self, output_file: str = 'validator_contagion_graph.json'):\n",
    "    \"\"\"Export validator contagion relationships as network graph.\"\"\"\n",
    "    if not self.hotspots:\n",
    "        print(\"⚠ No hotspots analyzed. Run identify_validator_hotspots() first.\")\n",
    "        return\n",
    "    \n",
    "    graph_data = {\n",
    "        'nodes': [],\n",
    "        'edges': [],\n",
    "        'metadata': {\n",
    "            'total_validators_analyzed': len(self.hotspots),\n",
    "            'total_records': len(self.mev_data) if self.mev_data is not None else 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for validator, hotspot in self.hotspots.items():\n",
    "        graph_data['nodes'].append({\n",
    "            'id': validator,\n",
    "            'type': 'validator',\n",
    "            'mev_count': hotspot.total_mev_count,\n",
    "            'concentration': hotspot.concentration_ratio,\n",
    "            'risk_level': hotspot.risk_level\n",
    "        })\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(graph_data, f, indent=2)\n",
    "    \n",
    "    print(f\"✓ Exported contagion graph to {output_file}\")\n",
    "    return graph_data\n",
    "\n",
    "def generate_summary_report(self) -> Dict[str, Any]:\n",
    "    \"\"\"Generate comprehensive summary report of contagion analysis.\"\"\"\n",
    "    report = {\n",
    "        'analysis_timestamp': pd.Timestamp.now().isoformat(),\n",
    "        'data_records': len(self.mev_data) if self.mev_data is not None else 0,\n",
    "        'validators_affected': self.mev_data['validator'].nunique() if self.mev_data is not None else 0,\n",
    "        'unique_attackers': self.mev_data['attacker_signer'].nunique() if self.mev_data is not None else 0,\n",
    "        'hotspots_identified': len(self.hotspots),\n",
    "        'key_findings': [],\n",
    "        'status': 'Analysis complete'\n",
    "    }\n",
    "    \n",
    "    if self.hotspots:\n",
    "        top_hotspot = list(self.hotspots.values())[0]\n",
    "        report['key_findings'].append(\n",
    "            f\"Top validator {list(self.hotspots.keys())[0][:16]}... \"\n",
    "            f\"accounts for {top_hotspot.concentration_ratio*100:.1f}% of all MEV activity\"\n",
    "        )\n",
    "    \n",
    "    return report\n",
    "\n",
    "ValidatorContagionAnalyzer.export_contagion_graph = export_contagion_graph\n",
    "ValidatorContagionAnalyzer.generate_summary_report = generate_summary_report\n",
    "print(\"✓ Utility methods added\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cc6b9f",
   "metadata": {},
   "source": [
    "# Initialize analyzer\n",
    "analyzer = ValidatorContagionAnalyzer(\n",
    "    data_dir=\"../02_mev_detection\"\n",
    ")\n",
    "\n",
    "print(\"✓ Analyzer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed786013",
   "metadata": {},
   "source": [
    "# Load data\n",
    "try:\n",
    "    analyzer.load_mev_data()\n",
    "except FileNotFoundError:\n",
    "    print(\"⚠ Could not find MEV detection data. Please check the data path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b91c3dd",
   "metadata": {},
   "source": [
    "### Export and Summarize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a412753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results\n",
    "if analyzer.mev_data is not None:\n",
    "    analyzer.export_contagion_graph('validator_contagion_graph.json')\n",
    "    summary = analyzer.generate_summary_report()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ANALYSIS COMPLETE\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nSummary Report:\")\n",
    "    print(f\"  Timestamp: {summary['analysis_timestamp']}\")\n",
    "    print(f\"  Total Records: {summary['data_records']:,}\")\n",
    "    print(f\"  Validators Affected: {summary['validators_affected']}\")\n",
    "    print(f\"  Unique Attackers: {summary['unique_attackers']}\")\n",
    "    print(f\"  Hotspots Identified: {summary['hotspots_identified']}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
