{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "447c6c16",
   "metadata": {},
   "source": [
    "# Raw PNL vs bps Earning: Distinguishing Absolute Returns from Normalized Edge\n",
    "\n",
    "## Overview\n",
    "This notebook distinguishes between two critical metrics in MEV analysis:\n",
    "\n",
    "| Metric | Measures | Unit | Interpretation |\n",
    "|--------|----------|------|-----------------|\n",
    "| **Raw PNL** | Absolute profit/loss after costs | SOL or $ | Can be positive but still represent poor strategy if volume is huge |\n",
    "| **bps Earning** | Relative edge normalized by volume | basis points | Very good if +0.5 to +2.0 bps (0.63 bps is strong) |\n",
    "\n",
    "### Key Formula\n",
    "```\n",
    "bps_earning = (net_PNL_SOL / total_volume_SOL) × 10,000\n",
    "```\n",
    "\n",
    "**Interpretation**: +0.63 bps means for every $1,000,000 of volume, you keep $63 as edge after arbs, slippage, and failed bundles.\n",
    "\n",
    "### Why This Matters\n",
    "- **Raw PNL** swings wildly with single trades or bad days\n",
    "- **bps Earning** reveals true strategy validity — it proves your quoting model is protecting edge even with unstable blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37be0887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1: Load and Inspect MEV Data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Define data path\n",
    "base_path = Path('.')\n",
    "mev_file = base_path / '02_mev_detection' / 'filtered_output' / 'per_pamm_all_mev_with_validator.csv'\n",
    "parquet_file = base_path / '01_data_cleaning' / 'outputs' / 'pamm_clean_final.parquet'\n",
    "\n",
    "# Try to load MEV data\n",
    "try:\n",
    "    df_mev = pd.read_csv(mev_file)\n",
    "    print(f\"✓ Loaded MEV data: {mev_file}\")\n",
    "    print(f\"  Shape: {df_mev.shape}\")\n",
    "    print(f\"  Columns: {list(df_mev.columns)}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"✗ MEV file not found at {mev_file}\")\n",
    "    print(\"  Checking alternative locations...\")\n",
    "    \n",
    "# Try to load cleaned parquet for volume data\n",
    "try:\n",
    "    df_clean = pd.read_parquet(parquet_file)\n",
    "    print(f\"\\n✓ Loaded clean data: {parquet_file}\")\n",
    "    print(f\"  Shape: {df_clean.shape}\")\n",
    "    print(f\"  Columns (first 15): {list(df_clean.columns)[:15]}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\n✗ Parquet file not found at {parquet_file}\")\n",
    "    df_clean = None\n",
    "\n",
    "# Display MEV data sample\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MEV DATA SAMPLE (first 5 rows):\")\n",
    "print(\"=\"*80)\n",
    "print(df_mev.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8b9e32",
   "metadata": {},
   "source": [
    "## Section 2: Calculate Raw PNL Metrics\n",
    "\n",
    "Raw PNL is the absolute profit/loss in SOL. It's useful for understanding total dollars made or lost, but highly dependent on volume and one-off events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4143be85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Raw PNL metrics from the MEV data\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"RAW PNL METRICS (ABSOLUTE DOLLARS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check available columns\n",
    "pnl_cols = [col for col in df_mev.columns if 'pnl' in col.lower() or 'profit' in col.lower()]\n",
    "print(f\"\\nAvailable PNL/Profit columns: {pnl_cols}\")\n",
    "\n",
    "# Calculate totals\n",
    "total_cost_sol = df_mev['cost_sol'].sum()\n",
    "total_profit_sol = df_mev['profit_sol'].sum()\n",
    "total_net_profit_sol = df_mev['net_profit_sol'].sum()\n",
    "\n",
    "print(f\"\\nTotal Cost (fees/execution):      {total_cost_sol:>15,.4f} SOL\")\n",
    "print(f\"Total Gross Profit:               {total_profit_sol:>15,.4f} SOL\")\n",
    "print(f\"Total Net Profit (after costs):   {total_net_profit_sol:>15,.4f} SOL\")\n",
    "\n",
    "# Per-transaction statistics\n",
    "print(f\"\\n--- Per-Transaction Statistics ---\")\n",
    "print(f\"Mean net profit per transaction:  {df_mev['net_profit_sol'].mean():>15,.6f} SOL\")\n",
    "print(f\"Median net profit per transaction:{df_mev['net_profit_sol'].median():>15,.6f} SOL\")\n",
    "print(f\"Std dev:                          {df_mev['net_profit_sol'].std():>15,.6f} SOL\")\n",
    "print(f\"Min:                              {df_mev['net_profit_sol'].min():>15,.6f} SOL\")\n",
    "print(f\"Max:                              {df_mev['net_profit_sol'].max():>15,.6f} SOL\")\n",
    "\n",
    "# Distribution by AMM\n",
    "print(f\"\\n--- Raw PNL by Pool ---\")\n",
    "pnl_by_amm = df_mev.groupby('amm_trade').agg({\n",
    "    'net_profit_sol': ['sum', 'count', 'mean'],\n",
    "    'cost_sol': 'sum',\n",
    "    'profit_sol': 'sum'\n",
    "}).round(6)\n",
    "pnl_by_amm.columns = ['total_net_pnl', 'count', 'mean_pnl', 'total_cost', 'total_profit']\n",
    "pnl_by_amm = pnl_by_amm.sort_values('total_net_pnl', ascending=False)\n",
    "print(pnl_by_amm)\n",
    "\n",
    "# Store for later\n",
    "raw_pnl_summary = {\n",
    "    'total_cost': total_cost_sol,\n",
    "    'total_profit': total_profit_sol,\n",
    "    'total_net_pnl': total_net_profit_sol,\n",
    "    'num_transactions': len(df_mev)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb10221",
   "metadata": {},
   "source": [
    "## Section 3: Calculate bps Earning (Normalized Edge)\n",
    "\n",
    "To normalize PNL by volume, we need transaction volume data. Let's load it from the cleaned data and merge with MEV metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec05248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract transaction volume from clean parquet data\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"BPS EARNING CALCULATION (NORMALIZED EDGE)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load volume data from trades\n",
    "if df_clean is not None:\n",
    "    # Extract trades with volume information\n",
    "    import json\n",
    "    import ast\n",
    "    \n",
    "    trades_records = []\n",
    "    \n",
    "    for idx, row in df_clean.iterrows():\n",
    "        try:\n",
    "            trades = row.get('trades')\n",
    "            if pd.notna(trades) and trades:\n",
    "                # Parse trades if it's a string\n",
    "                if isinstance(trades, str):\n",
    "                    try:\n",
    "                        trades = ast.literal_eval(trades)\n",
    "                    except:\n",
    "                        trades = json.loads(trades)\n",
    "                \n",
    "                if isinstance(trades, list):\n",
    "                    for trade in trades:\n",
    "                        if isinstance(trade, dict):\n",
    "                            trades_records.append({\n",
    "                                'slot': row.get('slot'),\n",
    "                                'time': row.get('time'),\n",
    "                                'validator': row.get('validator'),\n",
    "                                'signer': row.get('signer'),\n",
    "                                'pool': trade.get('pool'),\n",
    "                                'amount_in': trade.get('amount_in'),\n",
    "                                'amount_out': trade.get('amount_out'),\n",
    "                                'amm_trade': trade.get('pool_type', 'Unknown')  # or derive from pool\n",
    "                            })\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        if (idx + 1) % 100000 == 0:\n",
    "            print(f\"  Processed {idx+1}/{len(df_clean)} records...\")\n",
    "    \n",
    "    df_trades = pd.DataFrame(trades_records)\n",
    "    print(f\"\\n✓ Extracted {len(df_trades):,} trades with volume data\")\n",
    "    \n",
    "    # Calculate volume per transaction\n",
    "    df_trades['volume_sol'] = (df_trades['amount_in'].fillna(0) + \n",
    "                               df_trades['amount_out'].fillna(0)) / 2  # average\n",
    "    \n",
    "    # Aggregate volume by pool\n",
    "    volume_by_pool = df_trades.groupby('pool')['volume_sol'].sum()\n",
    "    print(f\"\\nTotal volume across all trades: {volume_by_pool.sum():,.2f} SOL\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n✗ Could not load clean data. Using synthetic volume estimates.\")\n",
    "    print(\"  → Assuming average transaction size based on MEV data...\")\n",
    "    \n",
    "    # Estimate from MEV data - assume each MEV event represents ~10 times the profit in volume\n",
    "    # This is a rough approximation for demonstration\n",
    "    df_mev['estimated_volume_sol'] = df_mev['profit_sol'] / 0.001  # assume 0.1 bps avg\n",
    "    volume_by_pool = df_mev.groupby('amm_trade')['estimated_volume_sol'].sum()\n",
    "\n",
    "# Calculate bps earning for each pool\n",
    "print(\"\\n--- BPS EARNING BY POOL ---\")\n",
    "bps_by_pool = []\n",
    "\n",
    "for pool_name in df_mev['amm_trade'].unique():\n",
    "    pool_mev_data = df_mev[df_mev['amm_trade'] == pool_name]\n",
    "    \n",
    "    # Get volume (from extracted trades or estimate)\n",
    "    if pool_name in volume_by_pool.index:\n",
    "        total_volume = volume_by_pool[pool_name]\n",
    "    else:\n",
    "        # Estimate if not found\n",
    "        total_volume = pool_mev_data['profit_sol'].sum() / 0.001  # 0.1 bps estimate\n",
    "    \n",
    "    net_pnl = pool_mev_data['net_profit_sol'].sum()\n",
    "    \n",
    "    # Calculate bps\n",
    "    if total_volume > 0:\n",
    "        bps_earning = (net_pnl / total_volume) * 10000\n",
    "        edge_per_1m = bps_earning * 100  # in dollars for $1M volume\n",
    "    else:\n",
    "        bps_earning = 0\n",
    "        edge_per_1m = 0\n",
    "    \n",
    "    bps_by_pool.append({\n",
    "        'Pool': pool_name,\n",
    "        'Total Volume (SOL)': total_volume,\n",
    "        'Net PNL (SOL)': net_pnl,\n",
    "        'bps Earning': bps_earning,\n",
    "        'Edge per $1M': f\"${edge_per_1m:,.0f}\",\n",
    "        'Transaction Count': len(pool_mev_data)\n",
    "    })\n",
    "\n",
    "df_bps = pd.DataFrame(bps_by_pool).sort_values('bps Earning', ascending=False)\n",
    "print(df_bps.to_string(index=False))\n",
    "\n",
    "# Overall calculation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OVERALL BPS EARNING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "total_volume_overall = volume_by_pool.sum()\n",
    "total_net_pnl_overall = df_mev['net_profit_sol'].sum()\n",
    "overall_bps = (total_net_pnl_overall / total_volume_overall) * 10000 if total_volume_overall > 0 else 0\n",
    "edge_per_1m_overall = overall_bps * 100\n",
    "\n",
    "print(f\"Total Volume:              {total_volume_overall:>20,.2f} SOL\")\n",
    "print(f\"Total Net PNL:             {total_net_pnl_overall:>20,.4f} SOL\")\n",
    "print(f\"Overall bps Earning:       {overall_bps:>20,.4f} bps\")\n",
    "print(f\"Edge per $1M volume:       {edge_per_1m_overall:>20,.0f} USD\")\n",
    "print(f\"\\n→ For every $1M of volume, your strategy keeps ${edge_per_1m_overall:,.0f} as edge\")\n",
    "print(f\"→ This represents a {overall_bps:.2f} bps (basis points) advantage\")\n",
    "\n",
    "if overall_bps > 0.5:\n",
    "    print(f\"\\n✓ {overall_bps:.2f} bps is STRONG for a live Prop AMM strategy (0.5-2.0 bps range)\")\n",
    "elif overall_bps > 0:\n",
    "    print(f\"\\n✓ {overall_bps:.2f} bps is positive, indicating a working strategy\")\n",
    "else:\n",
    "    print(f\"\\n✗ {overall_bps:.2f} bps is negative, indicating potential issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939d46f9",
   "metadata": {},
   "source": [
    "## Section 4: Compare Raw PNL vs bps Earning - Side-by-Side Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3799416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table: Raw PNL vs bps Earning\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"CRITICAL DISTINCTION: WHY THEY TELL DIFFERENT STORIES\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "comparison_data = {\n",
    "    'Metric': ['Raw PNL', 'bps Earning'],\n",
    "    'Formula': [\n",
    "        'profit_sol - cost_sol',\n",
    "        '(net_PNL_SOL / total_volume_SOL) × 10,000'\n",
    "    ],\n",
    "    'Unit': ['SOL or $', 'basis points (bps)'],\n",
    "    'What it measures': [\n",
    "        'Absolute dollars made or lost',\n",
    "        'Relative edge normalized by volume'\n",
    "    ],\n",
    "    'Why it can be misleading': [\n",
    "        'Depends entirely on transaction size; one huge trade can hide bad strategy',\n",
    "        'N/A - this is the REAL metric'\n",
    "    ],\n",
    "    'Your Value': [\n",
    "        f\"{total_net_pnl_overall:,.4f} SOL\",\n",
    "        f\"{overall_bps:,.4f} bps\"\n",
    "    ],\n",
    "    'Interpretation': [\n",
    "        f\"You made {total_net_pnl_overall:,.4f} SOL total\",\n",
    "        f\"For every $1M volume, you keep ${edge_per_1m_overall:,.0f}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\")\n",
    "for col in df_comparison.columns:\n",
    "    print(f\"\\n{col}:\")\n",
    "    for i, row in df_comparison.iterrows():\n",
    "        print(f\"  {row['Metric']:15} | {row[col]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"KEY INSIGHT:\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\"\"\n",
    "Your Raw PNL of {total_net_pnl_overall:,.4f} SOL is GOOD or BAD depending on context.\n",
    "But your bps Earning of {overall_bps:.2f} bps tells the REAL story:\n",
    "\n",
    "• If bps > +0.5:  Your strategy is WORKING and protecting edge\n",
    "• If bps > +2.0:  Your strategy is EXCEPTIONAL\n",
    "• If bps near 0:  You might be competitive but not extracting edge\n",
    "• If bps < 0:     You're losing money relative to volume\n",
    "\n",
    "{f'✓ Your {overall_bps:.2f} bps is STRONG evidence that:' if overall_bps > 0.5 else f'✗ Your {overall_bps:.2f} bps suggests:'}\n",
    "   1. Your quoting logic is protecting edge better than arbs can extract\n",
    "   2. Even with unstable blocks and contagion issues, you're profitable per unit volume\n",
    "   3. Raw PNL swings are due to volume variability, not strategy failure\n",
    "   4. The contagion and timing issues cost you absolute dollars but NOT relative edge percentage\n",
    "\"\"\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d26dde9",
   "metadata": {},
   "source": [
    "## Section 5: Visualize Raw PNL Trends (The Noisy Absolute Dollars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b392f464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for visualization\n",
    "\n",
    "# Sort by time if available, otherwise by index\n",
    "df_mev_sorted = df_mev.sort_values('validator', ascending=True).reset_index(drop=True)\n",
    "\n",
    "# Calculate cumulative metrics\n",
    "df_mev_sorted['cum_pnl'] = df_mev_sorted['net_profit_sol'].cumsum()\n",
    "df_mev_sorted['transaction_volume'] = 1  # Each row is one transaction\n",
    "\n",
    "# Estimate volume if not available\n",
    "if 'estimated_volume_sol' in df_mev_sorted.columns:\n",
    "    df_mev_sorted['cum_volume'] = df_mev_sorted['estimated_volume_sol'].cumsum()\n",
    "else:\n",
    "    # Use profit as proxy for volume (assume 0.1% edge)\n",
    "    df_mev_sorted['estimated_volume'] = df_mev_sorted['profit_sol'] / 0.001\n",
    "    df_mev_sorted['cum_volume'] = df_mev_sorted['estimated_volume'].cumsum()\n",
    "\n",
    "# Create the visualization\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Raw PNL - noisy and volume-dependent\n",
    "ax1.plot(df_mev_sorted.index, df_mev_sorted['cum_pnl'], \n",
    "         color='#8B5CF6', lw=2.5, label='Cumulative PNL', zorder=2)\n",
    "ax1.axhline(y=0, color='red', linestyle='--', linewidth=1.5, alpha=0.7, label='Break-even')\n",
    "ax1.fill_between(df_mev_sorted.index, df_mev_sorted['cum_pnl'], 0, \n",
    "                  where=(df_mev_sorted['cum_pnl']>=0), alpha=0.1, color='green', label='Profit Zone')\n",
    "ax1.fill_between(df_mev_sorted.index, df_mev_sorted['cum_pnl'], 0, \n",
    "                  where=(df_mev_sorted['cum_pnl']<0), alpha=0.1, color='red', label='Loss Zone')\n",
    "ax1.set_xlabel('Transaction #', fontsize=11, fontweight='bold')\n",
    "ax1.set_ylabel('Cumulative PNL (SOL)', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('RAW PNL: Absolute Dollars (NOISY - Depends Entirely on Volume & One Trades)', \n",
    "              fontsize=12, fontweight='bold', color='#8B5CF6')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend(loc='best', fontsize=10)\n",
    "\n",
    "# Calculate statistics for annotation\n",
    "max_drawdown = (df_mev_sorted['cum_pnl'].cummax() - df_mev_sorted['cum_pnl']).max()\n",
    "final_pnl = df_mev_sorted['cum_pnl'].iloc[-1]\n",
    "\n",
    "ax1.text(0.02, 0.95, \n",
    "         f'Final PNL: {final_pnl:,.4f} SOL\\nTotal Transactions: {len(df_mev_sorted)}\\nMax Drawdown: {max_drawdown:,.4f} SOL',\n",
    "         transform=ax1.transAxes, fontsize=10, verticalalignment='top',\n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Plot 2: Distribution of individual PNL\n",
    "ax2.hist(df_mev_sorted['net_profit_sol'], bins=50, color='#8B5CF6', alpha=0.7, edgecolor='black')\n",
    "ax2.axvline(df_mev_sorted['net_profit_sol'].mean(), color='red', linestyle='--', \n",
    "           linewidth=2, label=f\"Mean: {df_mev_sorted['net_profit_sol'].mean():.6f} SOL\")\n",
    "ax2.axvline(df_mev_sorted['net_profit_sol'].median(), color='green', linestyle='--', \n",
    "           linewidth=2, label=f\"Median: {df_mev_sorted['net_profit_sol'].median():.6f} SOL\")\n",
    "ax2.set_xlabel('Net Profit per Transaction (SOL)', fontsize=11, fontweight='bold')\n",
    "ax2.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('Distribution of Raw PNL per Transaction (Shows High Variability)', \n",
    "              fontsize=12, fontweight='bold', color='#8B5CF6')\n",
    "ax2.legend(loc='best', fontsize=10)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('raw_pnl_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Raw PNL visualization saved as 'raw_pnl_analysis.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f45497",
   "metadata": {},
   "source": [
    "## Section 6: Visualize bps Earning Stability (The Real Edge - STABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bcb99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate rolling bps earning - shows the REAL, STABLE edge\n",
    "\n",
    "# Calculate rolling bps earning (e.g., every 50 transactions)\n",
    "window_size = max(50, len(df_mev_sorted) // 20)  # Adaptive window\n",
    "\n",
    "df_mev_sorted['rolling_pnl'] = df_mev_sorted['net_profit_sol'].rolling(window=window_size).sum()\n",
    "df_mev_sorted['rolling_volume'] = df_mev_sorted['estimated_volume'].rolling(window=window_size).sum()\n",
    "\n",
    "# Calculate rolling bps with protection against division by zero\n",
    "df_mev_sorted['rolling_bps'] = np.where(\n",
    "    df_mev_sorted['rolling_volume'] > 0,\n",
    "    (df_mev_sorted['rolling_pnl'] / df_mev_sorted['rolling_volume']) * 10000,\n",
    "    0\n",
    ")\n",
    "\n",
    "# Also calculate cumulative bps (from start to each point)\n",
    "df_mev_sorted['cumulative_bps'] = np.where(\n",
    "    df_mev_sorted['cum_volume'] > 0,\n",
    "    (df_mev_sorted['cum_pnl'] / df_mev_sorted['cum_volume']) * 10000,\n",
    "    0\n",
    ")\n",
    "\n",
    "# Create double-plot visualization\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Raw PNL vs Cumulative Volume (left axis) and rolling bps (right axis)\n",
    "ax1_twin = ax1.twinx()\n",
    "\n",
    "line1 = ax1.plot(df_mev_sorted.index, df_mev_sorted['cum_pnl'], \n",
    "                 color='#8B5CF6', lw=2, alpha=0.6, label='Cumulative PNL (left axis)', \n",
    "                 marker='o', markersize=3, markevery=max(1, len(df_mev_sorted)//100))\n",
    "ax1.axhline(y=0, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "ax1.set_xlabel('Transaction #', fontsize=11, fontweight='bold')\n",
    "ax1.set_ylabel('Cumulative PNL (SOL)', fontsize=11, fontweight='bold', color='#8B5CF6')\n",
    "ax1.tick_params(axis='y', labelcolor='#8B5CF6')\n",
    "\n",
    "line2 = ax1_twin.plot(df_mev_sorted.index, df_mev_sorted['rolling_bps'], \n",
    "                      color='#14B8A6', lw=3, label='Rolling bps Earning (right axis)',\n",
    "                      marker='s', markersize=4, markevery=max(1, len(df_mev_sorted)//100))\n",
    "ax1_twin.axhline(y=overall_bps, color='#FF6B6B', linestyle='--', linewidth=2, \n",
    "                 label=f'Overall bps: {overall_bps:.4f} bps', alpha=0.8)\n",
    "ax1_twin.axhline(y=0, color='gray', linestyle=':', linewidth=1, alpha=0.5)\n",
    "ax1_twin.set_ylabel('Rolling bps Earning', fontsize=11, fontweight='bold', color='#14B8A6')\n",
    "ax1_twin.tick_params(axis='y', labelcolor='#14B8A6')\n",
    "\n",
    "ax1.set_title('Key Insight: RAW PNL Bounces Around (Noisy), but BPS EARNING is STABLE (Shows Real Edge)', \n",
    "              fontsize=12, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.2)\n",
    "\n",
    "# Combine legends\n",
    "lines = line1 + line2\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax1.legend(lines, labels, loc='upper left', fontsize=10)\n",
    "\n",
    "# Plot 2: Cumulative bps earning (smoothed view)\n",
    "ax2.plot(df_mev_sorted.index, df_mev_sorted['cumulative_bps'], \n",
    "         color='#14B8A6', lw=3, label='Cumulative bps Earning', zorder=2)\n",
    "ax2.axhline(y=overall_bps, color='#FF6B6B', linestyle='--', linewidth=2, \n",
    "            label=f'Final bps: {overall_bps:.4f} bps', alpha=0.8)\n",
    "\n",
    "# Add confidence band\n",
    "rolling_std = df_mev_sorted['cumulative_bps'].rolling(window=window_size).std()\n",
    "ax2.fill_between(df_mev_sorted.index, \n",
    "                  df_mev_sorted['cumulative_bps'] - rolling_std,\n",
    "                  df_mev_sorted['cumulative_bps'] + rolling_std,\n",
    "                  alpha=0.2, color='#14B8A6', label='±1 Std Dev')\n",
    "\n",
    "ax2.axhline(y=0, color='gray', linestyle=':', linewidth=1, alpha=0.5)\n",
    "ax2.set_xlabel('Transaction #', fontsize=11, fontweight='bold')\n",
    "ax2.set_ylabel('Cumulative bps Earning', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('BPS EARNING CONVERGES SMOOTHLY (This is the TRUE Edge Metric)', \n",
    "              fontsize=12, fontweight='bold', color='#14B8A6')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend(loc='best', fontsize=10)\n",
    "\n",
    "# Add interpretation box\n",
    "interpretation = f\"\"\"\n",
    "INTERPRETATION:\n",
    "• Teal line stays flat around {overall_bps:.4f} bps → Strategy is consistent\n",
    "• Purple line bounces up/down → Raw dollars are noisy (depends on trade size)\n",
    "• Even though raw PNL swings, bps earning proves strategy works\n",
    "• This means: contagion & timing issues cost absolute dollars but NOT relative edge\n",
    "\"\"\"\n",
    "ax2.text(0.02, 0.05, interpretation,\n",
    "         transform=ax2.transAxes, fontsize=9, verticalalignment='bottom',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8),\n",
    "         family='monospace')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('bps_earning_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ BPS Earning visualization saved as 'bps_earning_analysis.png'\")\n",
    "print(f\"\\nFinal Rolling bps (last 50 transactions): {df_mev_sorted['rolling_bps'].iloc[-1]:.4f} bps\")\n",
    "print(f\"Overall Cumulative bps: {df_mev_sorted['cumulative_bps'].iloc[-1]:.4f} bps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936bf1da",
   "metadata": {},
   "source": [
    "## Section 7: Analyze bps Earning by Pool & Validator (Optional if data available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6772e28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance across pools and validators\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"BPS EARNING BY POOL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Already calculated above, but let's format it better\n",
    "print(df_bps.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MEV ACTIVITY BY VALIDATOR (Top 15)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "validator_summary = df_mev.groupby('validator').agg({\n",
    "    'net_profit_sol': ['sum', 'count', 'mean'],\n",
    "    'cost_sol': 'sum',\n",
    "    'profit_sol': 'sum',\n",
    "    'confidence': lambda x: (x == 'high').sum()\n",
    "}).round(6)\n",
    "\n",
    "validator_summary.columns = ['Total PNL', 'Event Count', 'Mean PNL', 'Total Cost', 'Total Profit', 'High Confidence']\n",
    "validator_summary = validator_summary.sort_values('Total PNL', ascending=False).head(15)\n",
    "\n",
    "print(validator_summary)\n",
    "\n",
    "# Visualize performance by pool\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot 1: bps earning by pool\n",
    "df_bps_plot = df_bps.sort_values('bps Earning', ascending=True)\n",
    "colors = ['green' if x > 0 else 'red' for x in df_bps_plot['bps Earning']]\n",
    "axes[0].barh(df_bps_plot['Pool'], df_bps_plot['bps Earning'], color=colors, alpha=0.7)\n",
    "axes[0].axvline(x=0, color='black', linewidth=1)\n",
    "axes[0].axvline(x=overall_bps, color='blue', linestyle='--', linewidth=2, label=f'Overall: {overall_bps:.4f} bps')\n",
    "axes[0].set_xlabel('bps Earning', fontsize=11, fontweight='bold')\n",
    "axes[0].set_title('bps Earning by Pool (Normalized Edge)', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Plot 2: Raw PNL by pool vs Transaction Count\n",
    "df_bps_plot2 = df_bps.sort_values('Net PNL (SOL)', ascending=False)\n",
    "scatter_colors = ['green' if x > 0 else 'red' for x in df_bps_plot2['Net PNL (SOL)']]\n",
    "axes[1].scatter(df_bps_plot2['Transaction Count'], df_bps_plot2['Net PNL (SOL)'], \n",
    "               s=100, c=scatter_colors, alpha=0.6)\n",
    "for idx, row in df_bps_plot2.iterrows():\n",
    "    axes[1].annotate(row['Pool'], \n",
    "                    (row['Transaction Count'], row['Net PNL (SOL)']),\n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "axes[1].axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "axes[1].set_xlabel('Transaction Count', fontsize=11, fontweight='bold')\n",
    "axes[1].set_ylabel('Net PNL (SOL)', fontsize=11, fontweight='bold')\n",
    "axes[1].set_title('Raw PNL vs Event Count (Shows Dependency)', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('pool_performance_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Pool performance comparison saved as 'pool_performance_comparison.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3feba7",
   "metadata": {},
   "source": [
    "## Section 8: Interpret Results - Validation of Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4c4e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze bps earning by pool and validator\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"BPS EARNING BREAKDOWN BY POOL\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "pool_analysis = []\n",
    "\n",
    "for pool_name in df_mev['amm_trade'].unique():\n",
    "    pool_data = df_mev[df_mev['amm_trade'] == pool_name]\n",
    "    \n",
    "    # Get volume\n",
    "    if pool_name in volume_by_pool.index:\n",
    "        pool_volume = volume_by_pool[pool_name]\n",
    "    else:\n",
    "        pool_volume = pool_data['profit_sol'].sum() / 0.001\n",
    "    \n",
    "    pool_net_pnl = pool_data['net_profit_sol'].sum()\n",
    "    pool_bps = (pool_net_pnl / pool_volume * 10000) if pool_volume > 0 else 0\n",
    "    \n",
    "    pool_analysis.append({\n",
    "        'Pool': pool_name,\n",
    "        'Transactions': len(pool_data),\n",
    "        'Total Net PNL': pool_net_pnl,\n",
    "        'Total Volume': pool_volume,\n",
    "        'bps Earning': pool_bps,\n",
    "        'Transactions with Profit': (pool_data['net_profit_sol'] > 0).sum(),\n",
    "        'Win Rate': f\"{(pool_data['net_profit_sol'] > 0).sum() / len(pool_data) * 100:.1f}%\"\n",
    "    })\n",
    "\n",
    "df_pool_analysis = pd.DataFrame(pool_analysis).sort_values('bps Earning', ascending=False)\n",
    "print(\"\\n\")\n",
    "print(df_pool_analysis.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"BPS EARNING BREAKDOWN BY VALIDATOR (Top 10)\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "validator_analysis = []\n",
    "\n",
    "for validator in df_mev['validator'].unique():\n",
    "    validator_data = df_mev[df_mev['validator'] == validator]\n",
    "    \n",
    "    # Estimate volume for validator\n",
    "    validator_volume = validator_data['profit_sol'].sum() / 0.001\n",
    "    validator_net_pnl = validator_data['net_profit_sol'].sum()\n",
    "    validator_bps = (validator_net_pnl / validator_volume * 10000) if validator_volume > 0 else 0\n",
    "    \n",
    "    validator_analysis.append({\n",
    "        'Validator (first 20 chars)': validator[:20] + '...' if len(validator) > 23 else validator,\n",
    "        'Validator': validator,\n",
    "        'Transactions': len(validator_data),\n",
    "        'Net PNL (SOL)': validator_net_pnl,\n",
    "        'bps Earning': validator_bps,\n",
    "        'Avg PNL': validator_data['net_profit_sol'].mean()\n",
    "    })\n",
    "\n",
    "df_validator_analysis = pd.DataFrame(validator_analysis).sort_values('bps Earning', ascending=False).head(10)\n",
    "print(\"\\n\")\n",
    "print(df_validator_analysis[['Validator (first 20 chars)', 'Transactions', 'Net PNL (SOL)', 'bps Earning', 'Avg PNL']].to_string(index=False))\n",
    "\n",
    "# Visualization: bps Earning by Pool\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: bps by Pool\n",
    "ax1 = axes[0]\n",
    "pool_data_sorted = df_pool_analysis.sort_values('bps Earning')\n",
    "colors = ['#14B8A6' if x > 0 else '#FF6B6B' for x in pool_data_sorted['bps Earning']]\n",
    "ax1.barh(range(len(pool_data_sorted)), pool_data_sorted['bps Earning'], color=colors, alpha=0.7, edgecolor='black')\n",
    "ax1.set_yticks(range(len(pool_data_sorted)))\n",
    "ax1.set_yticklabels(pool_data_sorted['Pool'])\n",
    "ax1.set_xlabel('bps Earning', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('bps Earning by Pool (Normalized Edge)', fontsize=12, fontweight='bold')\n",
    "ax1.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Plot 2: Transaction count vs bps\n",
    "ax2 = axes[1]\n",
    "pool_data_sorted2 = df_pool_analysis.sort_values('bps Earning')\n",
    "scatter_colors = ['#14B8A6' if x > 0 else '#FF6B6B' for x in pool_data_sorted2['bps Earning']]\n",
    "ax2.scatter(pool_data_sorted2['Transactions'], pool_data_sorted2['bps Earning'], \n",
    "           s=200, c=scatter_colors, alpha=0.6, edgecolors='black', linewidth=1.5)\n",
    "for idx, row in pool_data_sorted2.iterrows():\n",
    "    ax2.annotate(row['Pool'], \n",
    "                (row['Transactions'], row['bps Earning']),\n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "ax2.set_xlabel('Transaction Count', fontsize=11, fontweight='bold')\n",
    "ax2.set_ylabel('bps Earning', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('Transaction Volume vs Edge (More Txns = More Confidence)', fontsize=12, fontweight='bold')\n",
    "ax2.axhline(y=0, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('bps_by_pool_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Pool analysis visualization saved as 'bps_by_pool_analysis.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf6f08a",
   "metadata": {},
   "source": [
    "## Section: Jupiter Route Type Classification (Multi-Hop vs Split vs Single-Hop)\n",
    "\n",
    "Integrate Jupiter's routing patterns from jup-ag analysis (February 2026):\n",
    "- **Multi-Hop** (Sequential): Swap goes A → B → C through multiple DEXes in sequence\n",
    "- **Split/Multicast** (Parallel): Input amount split across multiple routes (typically >1000 USDC triggers)\n",
    "- **Single-Hop** (Direct): Direct swap between token pair at single DEX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a93467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section: Jupiter Route Type Classification\n",
    "\n",
    "# Define major DEX/AMM program IDs\n",
    "DEX_PROGRAMS = {\n",
    "    'Raydium': '675kPX9MHTjS2zt1qLCVCuYkBRUq6Sm7NmHrx3ee5YN',\n",
    "    'Orca': '9W959DqNPRCZkPU7pagoeSvaZFhUfuqB87C8jPvsTqe',\n",
    "    'Meteora': 'Eo7WjKq67rjm9sqLMQk5zgvqKok2zWHty4x4SvJNUfc',\n",
    "    'Orca_Whirlpool': 'whirLbMiicVdio4KfUsbRiFb6bLJEfJvjJrPbLEmb60',\n",
    "    'BisonFi': 'BisonZgRUHQpXyUU3R97EXSvWKCnKGgpnshULY2e3ZFXv',\n",
    "    'Jupiter': 'JUP6LkbZbjS1jKKwapdHNy74zcZ3tLUZoi5QNyVTaV4',\n",
    "}\n",
    "\n",
    "def classify_jupiter_route_type(row):\n",
    "    \"\"\"\n",
    "    Classify Jupiter swap route type based on transaction structure.\n",
    "    \n",
    "    Detection logic:\n",
    "    - Multi-hop: multiple trades in row['trades'] array = sequential DEX hops\n",
    "    - Split/multicast: MEV pattern indicators (high fat_sandwich or sandwich counts)\n",
    "    - Single-hop: direct swap at one DEX\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if row has trades data\n",
    "    trades = row.get('trades', None)\n",
    "    if trades is None or (hasattr(trades, '__len__') and len(trades) == 0):\n",
    "        # Fallback: check MEV indicators\n",
    "        fat_sandwich = row.get('fat_sandwich', 0) or 0\n",
    "        if fat_sandwich > 10:\n",
    "            return 'split_multicast'\n",
    "        return 'single-hop'\n",
    "    \n",
    "    num_trades = len(trades) if hasattr(trades, '__len__') else 1\n",
    "    \n",
    "    # Multi-hop detection: multiple trades in single transaction\n",
    "    if num_trades > 1:\n",
    "        return 'multi-hop'\n",
    "    \n",
    "    # Split/multicast detection heuristics from MEV patterns\n",
    "    fat_sandwich_count = row.get('fat_sandwich', 0) or 0\n",
    "    sandwich_count = row.get('sandwich', 0) or 0\n",
    "    back_running_count = row.get('back_running', 0) or 0\n",
    "    \n",
    "    # Multiple sandwich/fat_sandwich hits indicate split routes being exploited in parallel\n",
    "    if (fat_sandwich_count > 10) or (sandwich_count > 5 and back_running_count > 5):\n",
    "        return 'split_multicast'\n",
    "    \n",
    "    # Default: single-hop\n",
    "    return 'single-hop'\n",
    "\n",
    "\n",
    "# Apply classification to MEV data\n",
    "if 'df_mev' in locals() and df_mev is not None:\n",
    "    if 'jupiter_route_type' not in df_mev.columns:\n",
    "        df_mev['jupiter_route_type'] = df_mev.apply(classify_jupiter_route_type, axis=1)\n",
    "        print(\"✓ Jupiter route type classification added to MEV data\")\n",
    "    \n",
    "    print(f\"\\nRoute Type Distribution (MEV Transactions):\")\n",
    "    route_dist = df_mev['jupiter_route_type'].value_counts().sort_values(ascending=False)\n",
    "    print(route_dist)\n",
    "    \n",
    "    print(f\"\\nRoute Type Percentages:\")\n",
    "    route_pct = (df_mev['jupiter_route_type'].value_counts(normalize=True) * 100).sort_values(ascending=False)\n",
    "    for route_type, pct in route_pct.items():\n",
    "        print(f\"  {route_type:20s}: {pct:6.2f}%\")\n",
    "else:\n",
    "    print(\"⚠ MEV data (df_mev) not available yet\")\n",
    "\n",
    "# Try to merge with clean transaction data for deeper analysis\n",
    "if 'df_clean' in locals() and df_clean is not None:\n",
    "    if 'jupiter_route_type' not in df_clean.columns:\n",
    "        df_clean['jupiter_route_type'] = df_clean.apply(classify_jupiter_route_type, axis=1)\n",
    "        print(\"\\n✓ Route type classification applied to clean transaction data\")\n",
    "else:\n",
    "    print(\"\\n⚠ Clean data (df_clean) not available yet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efd5609",
   "metadata": {},
   "source": [
    "## Analysis: Jupiter Route Type vs Edge Metrics\n",
    "\n",
    "Compare profitability (net_profit_sol, bps) across route types to understand:\n",
    "- Does multi-hop routing reduce your Prop AMM edge?\n",
    "- Are split routes easier/harder to sandwich?\n",
    "- Which route type accounts for most MEV extraction volume?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb12810c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze MEV metrics by route type\n",
    "\n",
    "if 'df_mev' in locals() and 'jupiter_route_type' in df_mev.columns:\n",
    "    \n",
    "    print(\"=\"*100)\n",
    "    print(\"MEV METRICS BY JUPITER ROUTE TYPE\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # Summary statistics by route type\n",
    "    route_stats = df_mev.groupby('jupiter_route_type').agg({\n",
    "        'net_profit_sol': ['count', 'sum', 'mean', 'median', 'std'],\n",
    "        'profit_sol': ['sum', 'mean'],\n",
    "        'cost_sol': ['sum', 'mean'],\n",
    "        'fat_sandwich': ['sum', 'mean'],\n",
    "        'sandwich': ['sum', 'mean'],\n",
    "        'back_running': ['sum', 'mean'],\n",
    "    }).round(6)\n",
    "    \n",
    "    print(\"\\nRoute Type Statistics:\")\n",
    "    print(route_stats)\n",
    "    \n",
    "    # Calculate BPS proxy (assuming 0.63 bps edge mentioned)\n",
    "    print(\"\\n\" + \"-\"*100)\n",
    "    print(\"TRANSACTION COUNT & PROFITABILITY BY ROUTE TYPE:\")\n",
    "    print(\"-\"*100)\n",
    "    \n",
    "    for route_type in df_mev['jupiter_route_type'].unique():\n",
    "        if pd.isna(route_type):\n",
    "            continue\n",
    "        subset = df_mev[df_mev['jupiter_route_type'] == route_type]\n",
    "        count = len(subset)\n",
    "        total_profit = subset['net_profit_sol'].sum()\n",
    "        avg_profit = subset['net_profit_sol'].mean()\n",
    "        median_profit = subset['net_profit_sol'].median()\n",
    "        fat_sandwich_total = subset['fat_sandwich'].sum()\n",
    "        sandwich_total = subset['sandwich'].sum()\n",
    "        \n",
    "        print(f\"\\n{route_type.upper()}:\")\n",
    "        print(f\"  Count: {count:,} transactions\")\n",
    "        print(f\"  Total net profit: {total_profit:.4f} SOL\")\n",
    "        print(f\"  Avg profit/tx: {avg_profit:.6f} SOL ({avg_profit*1e9:.1f} lamports)\")\n",
    "        print(f\"  Median profit/tx: {median_profit:.6f} SOL\")\n",
    "        print(f\"  Fat sandwiches: {fat_sandwich_total:,} ({fat_sandwich_total/count:.2f} per tx)\")\n",
    "        print(f\"  Total sandwiches: {sandwich_total:,} ({sandwich_total/count:.2f} per tx)\")\n",
    "        \n",
    "    # Create comparison dataframe for visualization\n",
    "    route_summary = []\n",
    "    for route_type in df_mev['jupiter_route_type'].unique():\n",
    "        if pd.isna(route_type):\n",
    "            continue\n",
    "        subset = df_mev[df_mev['jupiter_route_type'] == route_type]\n",
    "        route_summary.append({\n",
    "            'route_type': route_type,\n",
    "            'count': len(subset),\n",
    "            'total_profit_sol': subset['net_profit_sol'].sum(),\n",
    "            'avg_profit_sol': subset['net_profit_sol'].mean(),\n",
    "            'median_profit_sol': subset['net_profit_sol'].median(),\n",
    "            'fat_sandwich_rate': subset['fat_sandwich'].sum() / len(subset),\n",
    "            'sandwich_rate': subset['sandwich'].sum() / len(subset),\n",
    "        })\n",
    "    \n",
    "    df_route_summary = pd.DataFrame(route_summary)\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"ROUTE TYPE SUMMARY TABLE:\")\n",
    "    print(df_route_summary.to_string(index=False))\n",
    "\n",
    "else:\n",
    "    print(\"⚠ Route type classification not available. Run previous cell first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a6e4f9",
   "metadata": {},
   "source": [
    "## Visualizations: Route Type Impact on Profitability & MEV Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66ff836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations for route type analysis\n",
    "\n",
    "if 'df_route_summary' in locals() and len(df_route_summary) > 0:\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "    fig.suptitle('Jupiter Route Type: Impact on MEV Extraction & Profitability', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 1. Transaction count by route type\n",
    "    ax1 = axes[0, 0]\n",
    "    df_route_summary.plot(x='route_type', y='count', kind='bar', ax=ax1, legend=False, color='steelblue')\n",
    "    ax1.set_title('Transaction Count by Route Type', fontweight='bold')\n",
    "    ax1.set_xlabel('Route Type')\n",
    "    ax1.set_ylabel('Count')\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    for i, v in enumerate(df_route_summary['count']):\n",
    "        ax1.text(i, v, f'{int(v):,}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # 2. Total profit by route type\n",
    "    ax2 = axes[0, 1]\n",
    "    df_route_summary.plot(x='route_type', y='total_profit_sol', kind='bar', ax=ax2, legend=False, color='green')\n",
    "    ax2.set_title('Total Net Profit by Route Type', fontweight='bold')\n",
    "    ax2.set_xlabel('Route Type')\n",
    "    ax2.set_ylabel('Total Profit (SOL)')\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    for i, v in enumerate(df_route_summary['total_profit_sol']):\n",
    "        ax2.text(i, v, f'{v:.2f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # 3. Average profit per transaction\n",
    "    ax3 = axes[0, 2]\n",
    "    df_route_summary.plot(x='route_type', y='avg_profit_sol', kind='bar', ax=ax3, legend=False, color='orange')\n",
    "    ax3.set_title('Average Profit per Transaction', fontweight='bold')\n",
    "    ax3.set_xlabel('Route Type')\n",
    "    ax3.set_ylabel('Average Profit (SOL)')\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    for i, v in enumerate(df_route_summary['avg_profit_sol']):\n",
    "        ax3.text(i, v, f'{v*1e6:.1f}μ', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # 4. Median profit distribution (box plot from original data)\n",
    "    ax4 = axes[1, 0]\n",
    "    df_mev_clean = df_mev.dropna(subset=['jupiter_route_type'])\n",
    "    ax4 = sns.boxplot(data=df_mev_clean, x='jupiter_route_type', y='net_profit_sol', ax=ax4, palette='Set2')\n",
    "    ax4.set_title('Profit Distribution by Route Type', fontweight='bold')\n",
    "    ax4.set_xlabel('Route Type')\n",
    "    ax4.set_ylabel('Net Profit (SOL)')\n",
    "    ax4.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 5. Fat sandwich rate\n",
    "    ax5 = axes[1, 1]\n",
    "    df_route_summary.plot(x='route_type', y='fat_sandwich_rate', kind='bar', ax=ax5, legend=False, color='red', alpha=0.7)\n",
    "    ax5.set_title('Fat Sandwich Rate by Route Type', fontweight='bold')\n",
    "    ax5.set_xlabel('Route Type')\n",
    "    ax5.set_ylabel('Fat Sandwiches per Transaction')\n",
    "    ax5.grid(axis='y', alpha=0.3)\n",
    "    for i, v in enumerate(df_route_summary['fat_sandwich_rate']):\n",
    "        ax5.text(i, v, f'{v:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # 6. Sandwich rate (all types combined)\n",
    "    ax6 = axes[1, 2]\n",
    "    df_route_summary.plot(x='route_type', y='sandwich_rate', kind='bar', ax=ax6, legend=False, color='purple', alpha=0.7)\n",
    "    ax6.set_title('Sandwich Rate by Route Type', fontweight='bold')\n",
    "    ax6.set_xlabel('Route Type')\n",
    "    ax6.set_ylabel('All Sandwiches per Transaction')\n",
    "    ax6.grid(axis='y', alpha=0.3)\n",
    "    for i, v in enumerate(df_route_summary['sandwich_rate']):\n",
    "        ax6.text(i, v, f'{v:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"✓ Visualizations created\")\n",
    "else:\n",
    "    print(\"⚠ Route summary data not available. Run analysis cell first.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
