{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6932ac4a",
   "metadata": {},
   "source": [
    "# PAMM Pool Cross-Comparison Analysis\n",
    "## Oracle Latency, Vulnerabilities & MEV Exposure\n",
    "\n",
    "Comprehensive analysis comparing PAMM pools across:\n",
    "- Oracle update latency\n",
    "- Trade latency metrics\n",
    "- MEV vulnerability scores\n",
    "- Sandwich attack exposure\n",
    "- Token pair distribution\n",
    "- Validator coordination risks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e209f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "# Paths\n",
    "base_path = Path('../')\n",
    "data_path = base_path / '01_data_cleaning/outputs/pamm_clean_final.parquet'\n",
    "mev_path = base_path / '02_mev_detection/per_pamm_all_mev_with_validator.csv'\n",
    "\n",
    "print(f\"Data path exists: {data_path.exists()}\")\n",
    "print(f\"MEV path exists: {mev_path.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bce731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_parquet(data_path)\n",
    "mev_df = pd.read_csv(mev_path)\n",
    "\n",
    "print(f\"✓ Main data: {len(df):,} records, {df.shape[1]} columns\")\n",
    "print(f\"✓ MEV data: {len(mev_df):,} records, {mev_df.shape[1]} columns\")\n",
    "print(f\"\\nMain data columns: {list(df.columns)}\")\n",
    "print(f\"\\nMEV data columns: {list(mev_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e111e0",
   "metadata": {},
   "source": [
    "## 1. Extract PAMM Pool Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525beaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract pool information from trades\n",
    "print(\"Extracting PAMM pool metrics...\")\n",
    "\n",
    "# Convert time column to datetime if needed\n",
    "if 'datetime' in df.columns:\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
    "\n",
    "# Extract trades data\n",
    "trades_list = []\n",
    "for idx, row in df.iterrows():\n",
    "    if pd.notna(row.get('trades')) and row['trades']:\n",
    "        trades = row['trades'] if isinstance(row['trades'], list) else [row['trades']]\n",
    "        for trade in trades:\n",
    "            if isinstance(trade, dict):\n",
    "                trade_record = {\n",
    "                    'slot': row['slot'],\n",
    "                    'time': row['time'],\n",
    "                    'datetime': row['datetime'],\n",
    "                    'validator': row['validator'],\n",
    "                    'signer': row['signer'],\n",
    "                    'pool': trade.get('pool'),\n",
    "                    'token_pair': trade.get('token_pair'),\n",
    "                    'amount_in': trade.get('amount_in'),\n",
    "                    'amount_out': trade.get('amount_out'),\n",
    "                    'us_since_first_shred': row.get('us_since_first_shred')\n",
    "                }\n",
    "                trades_list.append(trade_record)\n",
    "    if idx % 100000 == 0 and idx > 0:\n",
    "        print(f\"  Processed {idx:,} rows, extracted {len(trades_list):,} trades\")\n",
    "\n",
    "trades_df = pd.DataFrame(trades_list)\n",
    "print(f\"\\n✓ Extracted {len(trades_df):,} trades from {len(df):,} records\")\n",
    "print(f\"Columns: {list(trades_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300d4f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract oracle updates\n",
    "print(\"Extracting oracle update metrics...\")\n",
    "\n",
    "oracle_list = []\n",
    "for idx, row in df.iterrows():\n",
    "    if pd.notna(row.get('amm_oracle')) and row['amm_oracle']:\n",
    "        oracles = row['amm_oracle'] if isinstance(row['amm_oracle'], list) else [row['amm_oracle']]\n",
    "        for oracle in oracles:\n",
    "            if isinstance(oracle, dict):\n",
    "                oracle_record = {\n",
    "                    'slot': row['slot'],\n",
    "                    'time': row['time'],\n",
    "                    'datetime': row['datetime'],\n",
    "                    'validator': row['validator'],\n",
    "                    'oracle_type': oracle.get('oracle_type'),\n",
    "                    'pool': oracle.get('pool'),\n",
    "                    'token_pair': oracle.get('token_pair'),\n",
    "                    'us_since_first_shred': row.get('us_since_first_shred')\n",
    "                }\n",
    "                oracle_list.append(oracle_record)\n",
    "    if idx % 100000 == 0 and idx > 0:\n",
    "        print(f\"  Processed {idx:,} rows, extracted {len(oracle_list):,} updates\")\n",
    "\n",
    "oracle_df = pd.DataFrame(oracle_list)\n",
    "print(f\"\\n✓ Extracted {len(oracle_df):,} oracle updates from {len(df):,} records\")\n",
    "print(f\"Columns: {list(oracle_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec98487",
   "metadata": {},
   "source": [
    "## 2. Calculate Oracle Latency Metrics by Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eab22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate oracle latency per pool\n",
    "print(\"Calculating oracle latency metrics...\")\n",
    "\n",
    "oracle_latency = oracle_df.groupby(['pool', 'token_pair']).agg({\n",
    "    'us_since_first_shred': ['count', 'mean', 'median', 'std', 'min', 'max'],\n",
    "    'time': lambda x: (x.max() - x.min()) if len(x) > 1 else 0\n",
    "}).round(2)\n",
    "\n",
    "oracle_latency.columns = ['update_count', 'mean_latency_us', 'median_latency_us', \n",
    "                          'std_latency_us', 'min_latency_us', 'max_latency_us', 'time_span']\n",
    "oracle_latency = oracle_latency.reset_index()\n",
    "oracle_latency = oracle_latency.sort_values('mean_latency_us', ascending=False)\n",
    "\n",
    "print(f\"Oracle latency metrics for {len(oracle_latency)} pool-pair combinations:\")\n",
    "print(oracle_latency.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267965c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate trade latency per pool\n",
    "print(\"Calculating trade latency metrics...\")\n",
    "\n",
    "trade_latency = trades_df.groupby(['pool', 'token_pair']).agg({\n",
    "    'us_since_first_shred': ['count', 'mean', 'median', 'std', 'min', 'max'],\n",
    "    'signer': 'nunique'\n",
    "}).round(2)\n",
    "\n",
    "trade_latency.columns = ['trade_count', 'mean_trade_latency_us', 'median_trade_latency_us', \n",
    "                         'std_trade_latency_us', 'min_trade_latency_us', 'max_trade_latency_us', 'unique_signers']\n",
    "trade_latency = trade_latency.reset_index()\n",
    "trade_latency = trade_latency.sort_values('mean_trade_latency_us', ascending=False)\n",
    "\n",
    "print(f\"Trade latency metrics for {len(trade_latency)} pool-pair combinations:\")\n",
    "print(trade_latency.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd750a3",
   "metadata": {},
   "source": [
    "## 3. Calculate MEV Vulnerability Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1200d71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate MEV metrics per pool\n",
    "print(\"Calculating MEV vulnerability metrics per pool...\")\n",
    "\n",
    "mev_aggregated = mev_df.agg({\n",
    "    'back_running': 'sum',\n",
    "    'front_running': 'sum',\n",
    "    'sandwich': 'sum',\n",
    "    'fat_sandwich': 'sum',\n",
    "    'sandwich_complete': 'sum',\n",
    "    'cost_sol': 'sum',\n",
    "    'profit_sol': 'sum',\n",
    "    'net_profit_sol': 'sum',\n",
    "    'confidence': 'mean'\n",
    "})\n",
    "\n",
    "print(\"Total MEV Summary:\")\n",
    "print(mev_aggregated)\n",
    "\n",
    "# Per-pool MEV analysis\n",
    "# Extract pool from amm_trade if available, or create proxy\n",
    "mev_by_validator = mev_df.groupby('validator').agg({\n",
    "    'back_running': 'sum',\n",
    "    'front_running': 'sum',\n",
    "    'sandwich': 'sum',\n",
    "    'fat_sandwich': 'sum',\n",
    "    'sandwich_complete': 'sum',\n",
    "    'cost_sol': 'sum',\n",
    "    'profit_sol': 'sum',\n",
    "    'net_profit_sol': 'sum',\n",
    "    'confidence': 'mean',\n",
    "    'amm_trade': 'count'\n",
    "}).round(4)\n",
    "\n",
    "mev_by_validator.columns = ['back_running_count', 'front_running_count', 'sandwich_count', \n",
    "                            'fat_sandwich_count', 'sandwich_complete_count', 'total_cost_sol', \n",
    "                            'total_profit_sol', 'net_profit_sol', 'avg_confidence', 'mev_events']\n",
    "mev_by_validator = mev_by_validator.sort_values('net_profit_sol', ascending=False)\n",
    "\n",
    "print(f\"\\nMEV metrics by validator (top 15):\")\n",
    "print(mev_by_validator.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e806318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate vulnerability score per token pair\n",
    "print(\"Calculating vulnerability scores per token pair...\")\n",
    "\n",
    "# Get token pair info from trades\n",
    "if len(trades_df) > 0:\n",
    "    pair_metrics = trades_df.groupby('token_pair').agg({\n",
    "        'trade_count': 'sum' if 'trade_count' in trades_df.columns else 'size',\n",
    "        'us_since_first_shred': ['mean', 'median', 'std'],\n",
    "        'signer': 'nunique',\n",
    "        'validator': 'nunique',\n",
    "        'pool': 'nunique'\n",
    "    }).round(2)\n",
    "    \n",
    "    # Flatten columns\n",
    "    pair_metrics.columns = ['_'.join(col).strip('_') for col in pair_metrics.columns.values]\n",
    "    pair_metrics = pair_metrics.reset_index()\n",
    "    \n",
    "    # Rename for clarity\n",
    "    if 'us_since_first_shred_mean' in pair_metrics.columns:\n",
    "        pair_metrics.rename(columns={\n",
    "            'us_since_first_shred_mean': 'mean_latency_us',\n",
    "            'us_since_first_shred_median': 'median_latency_us',\n",
    "            'us_since_first_shred_std': 'std_latency_us',\n",
    "            'signer_nunique': 'unique_signers',\n",
    "            'validator_nunique': 'unique_validators',\n",
    "            'pool_nunique': 'unique_pools'\n",
    "        }, inplace=True)\n",
    "    \n",
    "    # Calculate vulnerability score (higher = more vulnerable)\n",
    "    pair_metrics['vulnerability_score'] = (\n",
    "        (pair_metrics['mean_latency_us'] / pair_metrics['mean_latency_us'].max() * 0.3) +  # Oracle latency factor\n",
    "        (pair_metrics['unique_validators'] / pair_metrics['unique_validators'].max() * 0.3) +  # Validator concentration\n",
    "        (pair_metrics['unique_signers'] / pair_metrics['unique_signers'].max() * 0.2) +  # Attacker diversity\n",
    "        (pair_metrics['unique_pools'] / pair_metrics['unique_pools'].max() * 0.2)  # Pool diversity\n",
    "    ).round(3)\n",
    "    \n",
    "    pair_metrics = pair_metrics.sort_values('vulnerability_score', ascending=False)\n",
    "    \n",
    "    print(f\"Vulnerability metrics for {len(pair_metrics)} token pairs:\")\n",
    "    print(pair_metrics.head(20))\n",
    "else:\n",
    "    print(\"No trade data available for token pair analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618b40e9",
   "metadata": {},
   "source": [
    "## 4. Create Comparison Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78974ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TABLE 1: Oracle Latency Comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TABLE 1: ORACLE LATENCY METRICS BY POOL-PAIR\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "oracle_latency_top = oracle_latency.head(20).copy()\n",
    "oracle_latency_top = oracle_latency_top[['pool', 'token_pair', 'update_count', \n",
    "                                          'mean_latency_us', 'median_latency_us', \n",
    "                                          'std_latency_us', 'max_latency_us']]\n",
    "print(oracle_latency_top.to_string(index=False))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c27c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TABLE 2: Trade Latency Comparison  \n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TABLE 2: TRADE LATENCY METRICS BY POOL-PAIR\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "trade_latency_top = trade_latency.head(20).copy()\n",
    "trade_latency_top = trade_latency_top[['pool', 'token_pair', 'trade_count', \n",
    "                                        'mean_trade_latency_us', 'median_trade_latency_us', \n",
    "                                        'std_trade_latency_us', 'unique_signers']]\n",
    "print(trade_latency_top.to_string(index=False))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a24eaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TABLE 3: Token Pair Vulnerability Scores\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TABLE 3: TOKEN PAIR VULNERABILITY ASSESSMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(pair_metrics) > 0:\n",
    "    pair_vuln_display = pair_metrics.head(20).copy()\n",
    "    pair_vuln_display = pair_vuln_display[['token_pair', 'mean_latency_us', \n",
    "                                           'unique_validators', 'unique_signers', \n",
    "                                           'unique_pools', 'vulnerability_score']]\n",
    "    print(pair_vuln_display.to_string(index=False))\n",
    "else:\n",
    "    print(\"No vulnerability metrics available\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923d3d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TABLE 4: MEV Risk by Validator\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TABLE 4: MEV RISK ASSESSMENT BY VALIDATOR\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "mev_display = mev_by_validator.head(15).copy()\n",
    "mev_display = mev_display.reset_index()\n",
    "mev_display = mev_display[['validator', 'mev_events', 'sandwich_count', \n",
    "                           'fat_sandwich_count', 'net_profit_sol', 'avg_confidence']]\n",
    "print(mev_display.to_string(index=False))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ed114f",
   "metadata": {},
   "source": [
    "## 5. Visualization: Oracle Latency Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c5d5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart 1: Top pools by oracle latency\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Sort by mean latency and take top 15\n",
    "oracle_top15 = oracle_latency.nlargest(15, 'mean_latency_us').copy()\n",
    "oracle_top15['pool_pair'] = oracle_top15['pool'].str[:8] + '...' + oracle_top15['token_pair']\n",
    "\n",
    "# Plot 1: Mean latency\n",
    "ax1 = axes[0]\n",
    "bars1 = ax1.barh(range(len(oracle_top15)), oracle_top15['mean_latency_us'], color='steelblue')\n",
    "ax1.set_yticks(range(len(oracle_top15)))\n",
    "ax1.set_yticklabels(oracle_top15['pool_pair'], fontsize=9)\n",
    "ax1.set_xlabel('Mean Oracle Latency (microseconds)', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('TOP 15 POOLS: Mean Oracle Update Latency', fontsize=12, fontweight='bold')\n",
    "ax1.invert_yaxis()\n",
    "for i, (idx, row) in enumerate(oracle_top15.iterrows()):\n",
    "    ax1.text(row['mean_latency_us'], i, f\" {row['mean_latency_us']:.0f}µs\", \n",
    "            va='center', fontsize=8)\n",
    "\n",
    "# Plot 2: Update frequency\n",
    "ax2 = axes[1]\n",
    "oracle_top15_freq = oracle_latency.nlargest(15, 'update_count').copy()\n",
    "oracle_top15_freq['pool_pair'] = oracle_top15_freq['pool'].str[:8] + '...' + oracle_top15_freq['token_pair']\n",
    "bars2 = ax2.barh(range(len(oracle_top15_freq)), oracle_top15_freq['update_count'], color='darkgreen')\n",
    "ax2.set_yticks(range(len(oracle_top15_freq)))\n",
    "ax2.set_yticklabels(oracle_top15_freq['pool_pair'], fontsize=9)\n",
    "ax2.set_xlabel('Number of Oracle Updates', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('TOP 15 POOLS: Oracle Update Frequency', fontsize=12, fontweight='bold')\n",
    "ax2.invert_yaxis()\n",
    "for i, (idx, row) in enumerate(oracle_top15_freq.iterrows()):\n",
    "    ax2.text(row['update_count'], i, f\" {int(row['update_count'])}\", \n",
    "            va='center', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('oracle_latency_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Saved: oracle_latency_comparison.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a6c14c",
   "metadata": {},
   "source": [
    "## 6. Visualization: Trade Latency Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69848748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart 2: Trade latency comparison\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Mean trade latency\n",
    "trade_top15 = trade_latency.nlargest(15, 'mean_trade_latency_us').copy()\n",
    "trade_top15['pool_pair'] = trade_top15['pool'].str[:8] + '...' + trade_top15['token_pair']\n",
    "\n",
    "ax1 = axes[0]\n",
    "bars1 = ax1.barh(range(len(trade_top15)), trade_top15['mean_trade_latency_us'], color='coral')\n",
    "ax1.set_yticks(range(len(trade_top15)))\n",
    "ax1.set_yticklabels(trade_top15['pool_pair'], fontsize=9)\n",
    "ax1.set_xlabel('Mean Trade Latency (microseconds)', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('TOP 15 POOLS: Mean Trade Execution Latency', fontsize=12, fontweight='bold')\n",
    "ax1.invert_yaxis()\n",
    "for i, (idx, row) in enumerate(trade_top15.iterrows()):\n",
    "    ax1.text(row['mean_trade_latency_us'], i, f\" {row['mean_trade_latency_us']:.0f}µs\", \n",
    "            va='center', fontsize=8)\n",
    "\n",
    "# Plot 2: Trade volume\n",
    "ax2 = axes[1]\n",
    "trade_top15_vol = trade_latency.nlargest(15, 'trade_count').copy()\n",
    "trade_top15_vol['pool_pair'] = trade_top15_vol['pool'].str[:8] + '...' + trade_top15_vol['token_pair']\n",
    "bars2 = ax2.barh(range(len(trade_top15_vol)), trade_top15_vol['trade_count'], color='darkred')\n",
    "ax2.set_yticks(range(len(trade_top15_vol)))\n",
    "ax2.set_yticklabels(trade_top15_vol['pool_pair'], fontsize=9)\n",
    "ax2.set_xlabel('Number of Trades', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('TOP 15 POOLS: Trade Volume', fontsize=12, fontweight='bold')\n",
    "ax2.invert_yaxis()\n",
    "for i, (idx, row) in enumerate(trade_top15_vol.iterrows()):\n",
    "    ax2.text(row['trade_count'], i, f\" {int(row['trade_count'])}\", \n",
    "            va='center', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('trade_latency_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Saved: trade_latency_comparison.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f4db2e",
   "metadata": {},
   "source": [
    "## 7. Visualization: Vulnerability Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8f575c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "723c1b52",
   "metadata": {},
   "source": [
    "## 8. Visualization: MEV Risk Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4694fa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart 4: MEV Risk by Validator and Attack Type\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "mev_top12 = mev_by_validator.head(12).reset_index()\n",
    "\n",
    "# Plot 1: MEV events by type\n",
    "ax1 = axes[0, 0]\n",
    "x_pos = np.arange(len(mev_top12))\n",
    "width = 0.2\n",
    "ax1.bar(x_pos - 1.5*width, mev_top12['back_running_count'], width, label='Back-run', alpha=0.8)\n",
    "ax1.bar(x_pos - 0.5*width, mev_top12['front_running_count'], width, label='Front-run', alpha=0.8)\n",
    "ax1.bar(x_pos + 0.5*width, mev_top12['sandwich_count'], width, label='Sandwich', alpha=0.8)\n",
    "ax1.bar(x_pos + 1.5*width, mev_top12['fat_sandwich_count'], width, label='Fat Sandwich', alpha=0.8)\n",
    "ax1.set_xlabel('Validator', fontsize=11, fontweight='bold')\n",
    "ax1.set_ylabel('Attack Count', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('MEV Attack Types by Top Validators', fontsize=12, fontweight='bold')\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(mev_top12['validator'].str[:6], rotation=45, ha='right', fontsize=8)\n",
    "ax1.legend(loc='upper right')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 2: Net Profit by Validator\n",
    "ax2 = axes[0, 1]\n",
    "colors_profit = ['green' if x > 0 else 'red' for x in mev_top12['net_profit_sol']]\n",
    "bars2 = ax2.barh(range(len(mev_top12)), mev_top12['net_profit_sol'], color=colors_profit)\n",
    "ax2.set_yticks(range(len(mev_top12)))\n",
    "ax2.set_yticklabels(mev_top12['validator'], fontsize=9)\n",
    "ax2.set_xlabel('Net Profit (SOL)', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('MEV Net Profit by Top Validators', fontsize=12, fontweight='bold')\n",
    "ax2.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "# Plot 3: Total Cost vs Profit\n",
    "ax3 = axes[1, 0]\n",
    "scatter3 = ax3.scatter(mev_top12['total_cost_sol'], mev_top12['total_profit_sol'],\n",
    "                       s=mev_top12['mev_events']*5, alpha=0.6,\n",
    "                       c=mev_top12['net_profit_sol'], cmap='RdYlGn')\n",
    "ax3.set_xlabel('Total Cost (SOL)', fontsize=11, fontweight='bold')\n",
    "ax3.set_ylabel('Total Profit (SOL)', fontsize=11, fontweight='bold')\n",
    "ax3.set_title('MEV Cost vs Profit (bubble size = event count)', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(scatter3, ax=ax3, label='Net Profit')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Confidence level\n",
    "ax4 = axes[1, 1]\n",
    "bars4 = ax4.barh(range(len(mev_top12)), mev_top12['avg_confidence'], color='steelblue')\n",
    "ax4.set_yticks(range(len(mev_top12)))\n",
    "ax4.set_yticklabels(mev_top12['validator'], fontsize=9)\n",
    "ax4.set_xlabel('Average Detection Confidence', fontsize=11, fontweight='bold')\n",
    "ax4.set_title('MEV Detection Confidence by Validator', fontsize=12, fontweight='bold')\n",
    "ax4.invert_yaxis()\n",
    "for i, (idx, row) in enumerate(mev_top12.iterrows()):\n",
    "    ax4.text(row['avg_confidence'], i, f\" {row['avg_confidence']:.3f}\", \n",
    "            va='center', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('mev_risk_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Saved: mev_risk_analysis.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afa88af",
   "metadata": {},
   "source": [
    "## 9. Export Summary Tables to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45468ea3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9391674",
   "metadata": {},
   "source": [
    "## 10. Summary Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cc6bb9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
