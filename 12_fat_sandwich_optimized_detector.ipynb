{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a82a2a6",
   "metadata": {},
   "source": [
    "# Fat Sandwich vs Multi-Hop Arbitrage Detector (Optimized)\n",
    "\n",
    "## Single unified detector combining:\n",
    "- âœ… Fat sandwich detection (rolling time windows)\n",
    "- âœ… Attack classification (Fat Sandwich vs Multi-Hop Arbitrage)  \n",
    "- âœ… Comprehensive analysis with real data\n",
    "\n",
    "## Removes duplications:\n",
    "- 10_advanced_FP_solution notebooks (removed)\n",
    "- FAT_SANDWICH_VS_MULTIHOP_CLASSIFICATION.md (consolidated)\n",
    "- improved_fat_sandwich_detection.py (integrated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9eb224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ“ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da1c8ef",
   "metadata": {},
   "source": [
    "## 1. Load Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a7be29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned data\n",
    "data_path = '01_data_cleaning/outputs/pamm_clean_final.parquet'\n",
    "print(f\"Loading from {data_path}...\")\n",
    "\n",
    "df_clean = pd.read_parquet(data_path)\n",
    "print(f\"âœ“ Loaded {len(df_clean):,} transaction records\")\n",
    "print(f\"âœ“ Memory usage: {df_clean.memory_usage(deep=True).sum() / 1e9:.2f} GB\")\n",
    "print(f\"\\nData shape: {df_clean.shape}\")\n",
    "print(f\"\\nColumns: {list(df_clean.columns)}\")\n",
    "print(f\"\\nEvent types: {df_clean['kind'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acca68de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract TRADE events only\n",
    "df_trades = df_clean[df_clean['kind'] == 'TRADE'].copy().reset_index(drop=True)\n",
    "print(f\"âœ“ Filtered to {len(df_trades):,} TRADE events ({len(df_trades)/len(df_clean)*100:.1f}%)\")\n",
    "print(f\"\\nTrade data summary:\")\n",
    "df_trades.info(memory_usage='human')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ea75a3",
   "metadata": {},
   "source": [
    "## 2. Define Unified Detector Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea77af8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FatSandwichDetector:\n",
    "    \"\"\"Unified detector for Fat Sandwich and Multi-Hop Arbitrage patterns.\"\"\"\n",
    "    \n",
    "    def __init__(self, df_trades, verbose=True):\n",
    "        self.df_trades = df_trades.sort_values('ms_time').reset_index(drop=True)\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"âœ“ Initialized detector with {len(df_trades):,} trade events\")\n",
    "            print(f\"âœ“ Unique signers: {df_trades['signer'].nunique():,}\")\n",
    "            print(f\"âœ“ Time range: {df_trades['ms_time'].min()} to {df_trades['ms_time'].max()}\")\n",
    "    \n",
    "    def detect_fat_sandwiches(self, window_seconds=[1, 2, 5, 10], \n",
    "                               min_trades=5, max_victim_ratio=0.8, \n",
    "                               min_attacker_trades=2):\n",
    "        \"\"\"\n",
    "        Detect fat sandwich patterns using rolling time windows.\n",
    "        \n",
    "        A fat sandwich requires:\n",
    "        1. A-B-A pattern: Same attacker first and last\n",
    "        2. Wrapped victims: Different signers between attacker trades\n",
    "        3. Same token pair: Attacker trades reverse token pair\n",
    "        4. Time constraint: All within rolling window\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"FAT SANDWICH DETECTION: Rolling Time Windows\")\n",
    "            print(\"=\"*80)\n",
    "            print(f\"Parameters:\")\n",
    "            print(f\"  Windows: {window_seconds} seconds\")\n",
    "            print(f\"  Min trades/window: {min_trades}\")\n",
    "            print(f\"  Max victim ratio: {max_victim_ratio*100:.0f}%\")\n",
    "            print(f\"  Min attacker trades: {min_attacker_trades}\")\n",
    "            print(f\"\\nScanning...\\n\")\n",
    "        \n",
    "        fat_sandwiches = []\n",
    "        stats = {w: 0 for w in window_seconds}\n",
    "        stats.update({\n",
    "            'total_windows_checked': 0,\n",
    "            'passed_aba_pattern': 0,\n",
    "            'passed_victim_ratio': 0,\n",
    "            'high_confidence': 0,\n",
    "            'medium_confidence': 0\n",
    "        })\n",
    "        \n",
    "        # Group by AMM pool\n",
    "        amm_groups = list(self.df_trades.groupby('amm_trade')) if 'amm_trade' in self.df_trades.columns else [('All', self.df_trades)]\n",
    "        \n",
    "        for amm_name, amm_trades in amm_groups:\n",
    "            amm_trades = amm_trades.sort_values('ms_time').reset_index(drop=True)\n",
    "            \n",
    "            for window_sec in window_seconds:\n",
    "                window_ms = window_sec * 1000\n",
    "                i = 0\n",
    "                \n",
    "                while i < len(amm_trades):\n",
    "                    start_time = amm_trades.iloc[i]['ms_time']\n",
    "                    end_time = start_time + window_ms\n",
    "                    \n",
    "                    # Get trades in window\n",
    "                    window_mask = (amm_trades['ms_time'] >= start_time) & (amm_trades['ms_time'] <= end_time)\n",
    "                    window_trades = amm_trades[window_mask].copy()\n",
    "                    \n",
    "                    stats['total_windows_checked'] += 1\n",
    "                    \n",
    "                    if len(window_trades) < min_trades:\n",
    "                        i += 1\n",
    "                        continue\n",
    "                    \n",
    "                    signers = window_trades['signer'].tolist()\n",
    "                    \n",
    "                    # A-B-A Pattern: attacker appears first and last\n",
    "                    if signers[0] != signers[-1]:\n",
    "                        i += 1\n",
    "                        continue\n",
    "                    \n",
    "                    stats['passed_aba_pattern'] += 1\n",
    "                    attacker = signers[0]\n",
    "                    middle_signers = set(signers[1:-1])\n",
    "                    attacker_count = signers.count(attacker)\n",
    "                    \n",
    "                    # Validation checks\n",
    "                    if (attacker_count < min_attacker_trades or \n",
    "                        len(middle_signers) == 0 or \n",
    "                        attacker in middle_signers):\n",
    "                        i += 1\n",
    "                        continue\n",
    "                    \n",
    "                    # Victim ratio filter (avoid aggregator routing)\n",
    "                    victim_ratio = len(middle_signers) / len(window_trades)\n",
    "                    if victim_ratio > max_victim_ratio:\n",
    "                        i += 1\n",
    "                        continue\n",
    "                    \n",
    "                    stats['passed_victim_ratio'] += 1\n",
    "                    \n",
    "                    # Token pair validation\n",
    "                    token_pair_valid = True\n",
    "                    if 'from_token' in window_trades.columns and 'to_token' in window_trades.columns:\n",
    "                        attacker_trades = window_trades[window_trades['signer'] == attacker]\n",
    "                        if len(attacker_trades) >= 2:\n",
    "                            first_trade = attacker_trades.iloc[0]\n",
    "                            last_trade = attacker_trades.iloc[-1]\n",
    "                            first_pair = (first_trade['from_token'], first_trade['to_token'])\n",
    "                            last_pair = (last_trade['from_token'], last_trade['to_token'])\n",
    "                            if first_pair[0] != last_pair[1] or first_pair[1] != last_pair[0]:\n",
    "                                token_pair_valid = False\n",
    "                    \n",
    "                    if not token_pair_valid:\n",
    "                        i += 1\n",
    "                        continue\n",
    "                    \n",
    "                    # Confidence scoring\n",
    "                    conf_score = 0\n",
    "                    reasons = []\n",
    "                    \n",
    "                    if victim_ratio < 0.3:\n",
    "                        conf_score += 3\n",
    "                        reasons.append('low_victim_ratio')\n",
    "                    elif victim_ratio < 0.5:\n",
    "                        conf_score += 2\n",
    "                    \n",
    "                    if attacker_count >= 3:\n",
    "                        conf_score += 2\n",
    "                        reasons.append('multiple_attacker_trades')\n",
    "                    \n",
    "                    if token_pair_valid:\n",
    "                        conf_score += 2\n",
    "                        reasons.append('token_pair_reversal')\n",
    "                    \n",
    "                    if window_sec <= 2:\n",
    "                        conf_score += 1\n",
    "                        reasons.append('short_window')\n",
    "                    \n",
    "                    if len(middle_signers) >= 3:\n",
    "                        conf_score += 1\n",
    "                        reasons.append('multiple_victims')\n",
    "                    \n",
    "                    # Determine confidence level\n",
    "                    if conf_score >= 6:\n",
    "                        confidence = 'high'\n",
    "                        stats['high_confidence'] += 1\n",
    "                    elif conf_score >= 4:\n",
    "                        confidence = 'medium'\n",
    "                        stats['medium_confidence'] += 1\n",
    "                    else:\n",
    "                        confidence = 'low'\n",
    "                    \n",
    "                    stats[window_sec] += 1\n",
    "                    \n",
    "                    # Record detection\n",
    "                    fat_sandwiches.append({\n",
    "                        'amm_trade': amm_name,\n",
    "                        'attacker_signer': attacker,\n",
    "                        'victim_count': len(middle_signers),\n",
    "                        'victim_signers': list(middle_signers),\n",
    "                        'total_trades': len(window_trades),\n",
    "                        'attacker_trades': attacker_count,\n",
    "                        'victim_ratio': victim_ratio,\n",
    "                        'window_seconds': window_sec,\n",
    "                        'start_time_ms': start_time,\n",
    "                        'end_time_ms': min(end_time, window_trades.iloc[-1]['ms_time']),\n",
    "                        'actual_time_span_ms': window_trades.iloc[-1]['ms_time'] - window_trades.iloc[0]['ms_time'],\n",
    "                        'start_slot': window_trades.iloc[0]['slot'] if 'slot' in window_trades.columns else None,\n",
    "                        'end_slot': window_trades.iloc[-1]['slot'] if 'slot' in window_trades.columns else None,\n",
    "                        'validator': window_trades.iloc[0]['validator'] if 'validator' in window_trades.columns else None,\n",
    "                        'confidence': confidence,\n",
    "                        'confidence_score': conf_score,\n",
    "                        'confidence_reasons': ','.join(reasons),\n",
    "                        'token_pair_validated': token_pair_valid\n",
    "                    })\n",
    "                    \n",
    "                    i += max(1, attacker_count // 2)\n",
    "        \n",
    "        results_df = pd.DataFrame(fat_sandwiches)\n",
    "        \n",
    "        if self.verbose:\n",
    "            self._print_detection_summary(results_df, stats, window_seconds)\n",
    "        \n",
    "        return results_df, stats\n",
    "    \n",
    "    def classify_attack(self, cluster_trades, attacker_signer):\n",
    "        \"\"\"\n",
    "        Classify a single attack as Fat Sandwich or Multi-Hop Arbitrage.\n",
    "        \n",
    "        Fat Sandwich indicators:\n",
    "        - Wrapped victims (A-B-A pattern with victims in middle)\n",
    "        - Same token pair throughout\n",
    "        - Low pool diversity\n",
    "        \n",
    "        Multi-Hop Arbitrage indicators:\n",
    "        - Cycle routing (token path returns to start)\n",
    "        - Multiple different token pairs\n",
    "        - High pool diversity\n",
    "        - No mandatory victims\n",
    "        \"\"\"\n",
    "        cluster_sorted = cluster_trades.sort_values('ms_time').reset_index(drop=True)\n",
    "        attacker_indices = cluster_sorted[cluster_sorted['signer'] == attacker_signer].index.tolist()\n",
    "        \n",
    "        if len(attacker_indices) < 2:\n",
    "            return {'attack_type': 'unknown', 'confidence': 0.0}\n",
    "        \n",
    "        # Victim count\n",
    "        first_idx = attacker_indices[0]\n",
    "        last_idx = attacker_indices[-1]\n",
    "        middle_trades = cluster_sorted.iloc[first_idx + 1:last_idx]\n",
    "        victim_signers = middle_trades['signer'].unique().tolist()\n",
    "        victim_signers = [v for v in victim_signers if v != attacker_signer]\n",
    "        has_victims = len(victim_signers) >= 2\n",
    "        \n",
    "        # Token structure\n",
    "        signer_trades = cluster_sorted[cluster_sorted['signer'] == attacker_signer].sort_values('ms_time')\n",
    "        if 'from_token' in signer_trades.columns and 'to_token' in signer_trades.columns:\n",
    "            token_pairs = signer_trades[['from_token', 'to_token']].apply(\n",
    "                lambda row: tuple(sorted([row['from_token'], row['to_token']])), axis=1\n",
    "            ).nunique()\n",
    "            same_pair = token_pairs == 1\n",
    "        else:\n",
    "            token_pairs = 1\n",
    "            same_pair = True\n",
    "        \n",
    "        # Pool diversity\n",
    "        unique_pools = signer_trades['amm_trade'].nunique() if 'amm_trade' in signer_trades.columns else 1\n",
    "        \n",
    "        # Cycle routing detection\n",
    "        is_cycle = False\n",
    "        if len(signer_trades) >= 2 and 'from_token' in signer_trades.columns:\n",
    "            token_path = [signer_trades.iloc[0]['from_token']]\n",
    "            for _, trade in signer_trades.iterrows():\n",
    "                token_path.append(trade['to_token'])\n",
    "            is_cycle = token_path[0] == token_path[-1] or token_path[-1] in ['SOL', 'USDC']\n",
    "        \n",
    "        # Scoring\n",
    "        fs_score = 0.0  # Fat Sandwich score\n",
    "        if has_victims:\n",
    "            fs_score += 0.35\n",
    "        if same_pair:\n",
    "            fs_score += 0.25\n",
    "        if unique_pools <= 2:\n",
    "            fs_score += 0.20\n",
    "        \n",
    "        mh_score = 0.0  # Multi-Hop score\n",
    "        if is_cycle:\n",
    "            mh_score += 0.35\n",
    "        if token_pairs >= 3:\n",
    "            mh_score += 0.25\n",
    "        if unique_pools >= 3:\n",
    "            mh_score += 0.20\n",
    "        if not has_victims:\n",
    "            mh_score += 0.20\n",
    "        \n",
    "        # Determine type\n",
    "        if fs_score > mh_score + 0.15:\n",
    "            attack_type = 'fat_sandwich'\n",
    "            confidence = min(fs_score, 1.0)\n",
    "        elif mh_score > fs_score + 0.15:\n",
    "            attack_type = 'multi_hop_arbitrage'\n",
    "            confidence = min(mh_score, 1.0)\n",
    "        else:\n",
    "            attack_type = 'ambiguous'\n",
    "            confidence = max(fs_score, mh_score)\n",
    "        \n",
    "        return {\n",
    "            'attack_type': attack_type,\n",
    "            'confidence': confidence,\n",
    "            'fat_sandwich_score': min(fs_score, 1.0),\n",
    "            'multi_hop_score': min(mh_score, 1.0),\n",
    "            'victim_count': len(victim_signers),\n",
    "            'token_pairs': token_pairs,\n",
    "            'unique_pools': unique_pools,\n",
    "            'is_cycle': is_cycle\n",
    "        }\n",
    "    \n",
    "    def classify_all_attacks(self, detected_attacks_df, show_progress=True, sample_size=None):\n",
    "        \"\"\"Classify all detected attacks.\"\"\"\n",
    "        results = []\n",
    "        df_to_classify = detected_attacks_df.copy()\n",
    "        \n",
    "        # Optional: sample for faster processing\n",
    "        if sample_size and len(df_to_classify) > sample_size:\n",
    "            df_to_classify = df_to_classify.sample(n=sample_size, random_state=42)\n",
    "        \n",
    "        total = len(df_to_classify)\n",
    "        \n",
    "        for i, (idx, attack_row) in enumerate(df_to_classify.iterrows()):\n",
    "            if show_progress and i % max(1, total // 10) == 0:\n",
    "                print(f\"  Classifying: {i}/{total}...\", end='\\r')\n",
    "            \n",
    "            # Extract cluster\n",
    "            if pd.isna(attack_row['start_slot']) or pd.isna(attack_row['end_slot']):\n",
    "                cluster = self.df_trades[\n",
    "                    (self.df_trades['ms_time'] >= attack_row['start_time_ms']) &\n",
    "                    (self.df_trades['ms_time'] <= attack_row['end_time_ms'])\n",
    "                ].copy()\n",
    "            else:\n",
    "                cluster = self.df_trades[\n",
    "                    (self.df_trades['slot'] >= attack_row['start_slot']) &\n",
    "                    (self.df_trades['slot'] <= attack_row['end_slot']) &\n",
    "                    (self.df_trades['ms_time'] >= attack_row['start_time_ms']) &\n",
    "                    (self.df_trades['ms_time'] <= attack_row['end_time_ms'])\n",
    "                ].copy()\n",
    "            \n",
    "            if len(cluster) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Classify\n",
    "            classification = self.classify_attack(cluster, attack_row['attacker_signer'])\n",
    "            \n",
    "            # Add to row\n",
    "            for key, value in classification.items():\n",
    "                attack_row[key] = value\n",
    "            \n",
    "            results.append(attack_row)\n",
    "        \n",
    "        classified_df = pd.DataFrame(results)\n",
    "        \n",
    "        if show_progress:\n",
    "            print(f\"  Completed: {total}/{total}        \")\n",
    "            if len(results) > 0:\n",
    "                print(\"\\nðŸ“Š Classification Results:\")\n",
    "                if 'attack_type' in classified_df.columns:\n",
    "                    counts = classified_df['attack_type'].value_counts()\n",
    "                    for atype, count in counts.items():\n",
    "                        pct = count / len(classified_df) * 100\n",
    "                        print(f\"   {atype:25s}: {count:>5,} ({pct:>5.1f}%)\")\n",
    "        \n",
    "        return classified_df\n",
    "    \n",
    "    def _print_detection_summary(self, results_df, stats, window_seconds):\n",
    "        print(f\"âœ“ Total detections: {len(results_df):,}\\n\")\n",
    "        print(\"By Time Window:\")\n",
    "        for w in window_seconds:\n",
    "            count = stats[w]\n",
    "            pct = (count / len(results_df) * 100) if len(results_df) > 0 else 0\n",
    "            print(f\"  {w}s: {count:>6,} ({pct:>5.1f}%)\")\n",
    "        \n",
    "        print(\"\\nBy Confidence:\")\n",
    "        print(f\"  High:   {stats['high_confidence']:>6,}\")\n",
    "        print(f\"  Medium: {stats['medium_confidence']:>6,}\")\n",
    "        low_conf = len(results_df) - stats['high_confidence'] - stats['medium_confidence']\n",
    "        print(f\"  Low:    {low_conf:>6,}\")\n",
    "        \n",
    "        print(\"\\nValidation Pass Rates:\")\n",
    "        total_checked = stats['total_windows_checked']\n",
    "        if total_checked > 0:\n",
    "            print(f\"  Windows checked: {total_checked:,}\")\n",
    "            aba_pct = stats['passed_aba_pattern']/total_checked*100 if total_checked > 0 else 0\n",
    "            print(f\"  A-B-A pattern: {stats['passed_aba_pattern']:,} ({aba_pct:.1f}%)\")\n",
    "            print(f\"  Victim ratio: {stats['passed_victim_ratio']:,}\")\n",
    "\n",
    "\n",
    "print(\"âœ“ Detector class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62a6d48",
   "metadata": {},
   "source": [
    "## 3. Initialize Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbb1abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize with df_trades\n",
    "detector = FatSandwichDetector(df_trades, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbda91b",
   "metadata": {},
   "source": [
    "## 4. Run Fat Sandwich Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323c2401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run detection with rolling time windows\n",
    "detected_sandwiches, detection_stats = detector.detect_fat_sandwiches(\n",
    "    window_seconds=[1, 2, 5, 10],\n",
    "    min_trades=5,\n",
    "    max_victim_ratio=0.8,\n",
    "    min_attacker_trades=2\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Detection complete\")\n",
    "print(f\"âœ“ Found {len(detected_sandwiches):,} potential fat sandwiches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d729ca8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample detections\n",
    "if len(detected_sandwiches) > 0:\n",
    "    print(\"\\nðŸ“‹ Sample Detections (first 5):\")\n",
    "    print(\"-\"*100)\n",
    "    display(detected_sandwiches[['attacker_signer', 'victim_count', 'total_trades', \n",
    "                                  'actual_time_span_ms', 'confidence', 'window_seconds']].head())\n",
    "else:\n",
    "    print(\"âš ï¸ No fat sandwiches detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7c3d6c",
   "metadata": {},
   "source": [
    "## 5. Classify Detected Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedeb02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(detected_sandwiches) > 0:\n",
    "    # Classify all attacks (or sample for speed)\n",
    "    # Use sample_size parameter to test with smaller subset first\n",
    "    classified_results = detector.classify_all_attacks(\n",
    "        detected_sandwiches,\n",
    "        show_progress=True,\n",
    "        sample_size=None  # Change to e.g. 1000 for faster testing\n",
    "    )\n",
    "    print(f\"\\nâœ“ Classified {len(classified_results):,} attacks\")\n",
    "else:\n",
    "    print(\"Cannot classify - no detections found\")\n",
    "    classified_results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c87a10c",
   "metadata": {},
   "source": [
    "## 6. Analysis & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74159b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(classified_results) > 0:\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"COMPREHENSIVE ANALYSIS RESULTS\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Attack Type Distribution:\")\n",
    "    attack_types = classified_results['attack_type'].value_counts()\n",
    "    for atype, count in attack_types.items():\n",
    "        pct = count / len(classified_results) * 100\n",
    "        bar = 'â–ˆ' * int(pct // 5)\n",
    "        print(f\"  {atype:25s}: {count:>5,} ({pct:>5.1f}%) {bar}\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Top 10 Most Active Attackers:\")\n",
    "    top_attackers = classified_results['attacker_signer'].value_counts().head(10)\n",
    "    for i, (attacker, count) in enumerate(top_attackers.items(), 1):\n",
    "        print(f\"  {i:2d}. {attacker[:44]:44s}: {count:>5,} attacks\")\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ Confidence Statistics:\")\n",
    "    avg_confidence = classified_results['confidence'].mean()\n",
    "    print(f\"  Average: {avg_confidence:.2%}\")\n",
    "    print(f\"  High (â‰¥0.8):    {(classified_results['confidence'] >= 0.8).sum():>6,} ({(classified_results['confidence'] >= 0.8).sum()/len(classified_results)*100:>5.1f}%)\")\n",
    "    print(f\"  Medium (0.5-0.8): {((classified_results['confidence'] >= 0.5) & (classified_results['confidence'] < 0.8)).sum():>6,} ({((classified_results['confidence'] >= 0.5) & (classified_results['confidence'] < 0.8)).sum()/len(classified_results)*100:>5.1f}%)\")\n",
    "    print(f\"  Low (<0.5):     {(classified_results['confidence'] < 0.5).sum():>6,} ({(classified_results['confidence'] < 0.5).sum()/len(classified_results)*100:>5.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nâ±ï¸ Time Span Statistics:\")\n",
    "    time_span_seconds = classified_results['actual_time_span_ms'] / 1000\n",
    "    print(f\"  Average: {time_span_seconds.mean():.2f}s\")\n",
    "    print(f\"  Median:  {time_span_seconds.median():.2f}s\")\n",
    "    print(f\"  Max:     {time_span_seconds.max():.2f}s\")\n",
    "    print(f\"  Min:     {time_span_seconds.min():.2f}s\")\n",
    "    \n",
    "    print(f\"\\nðŸŠ Victim Statistics:\")\n",
    "    print(f\"  Avg victims/attack: {classified_results['victim_count'].mean():.1f}\")\n",
    "    print(f\"  Max victims: {classified_results['victim_count'].max()}\")\n",
    "    print(f\"  Total unique: {len(set([v for victims in classified_results.get('victim_signers', [[]]) for v in victims] if isinstance(victims, list)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880074bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show detailed sample results\n",
    "if len(classified_results) > 0:\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"DETAILED SAMPLE RESULTS (First 5 Attacks)\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    for i, row in classified_results.head(5).iterrows():\n",
    "        print(f\"\\nðŸ” Attack #{i+1}:\")\n",
    "        print(f\"  Attacker:        {row['attacker_signer'][:44]}\")\n",
    "        print(f\"  Attack Type:     {row['attack_type']}\")\n",
    "        print(f\"  Confidence:      {row['confidence']:.1%}\")\n",
    "        print(f\"  Fat Sandwich Score: {row['fat_sandwich_score']:.2f}\")\n",
    "        print(f\"  Multi-Hop Score:    {row['multi_hop_score']:.2f}\")\n",
    "        print(f\"  Victims:         {row.get('victim_count', 'N/A')}\")\n",
    "        print(f\"  Token Pairs:     {row.get('token_pairs', 'N/A')}\")\n",
    "        print(f\"  Pools Used:      {row.get('unique_pools', 'N/A')}\")\n",
    "        print(f\"  Time Span:       {row['actual_time_span_ms']/1000:.2f}s\")\n",
    "        print(f\"  Window Size:     {row['window_seconds']}s\")\n",
    "        print(f\"  Confidence Factors: {row.get('confidence_reasons', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff3d717",
   "metadata": {},
   "source": [
    "## 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fade66f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(classified_results) > 0:\n",
    "    # Save full results\n",
    "    output_path = 'fat_sandwich_classification_results.parquet'\n",
    "    classified_results.to_parquet(output_path)\n",
    "    print(f\"âœ“ Full results saved to {output_path}\")\n",
    "    \n",
    "    # Save summary CSV\n",
    "    summary_path = 'fat_sandwich_summary.csv'\n",
    "    summary_cols = ['attacker_signer', 'attack_type', 'confidence', 'fat_sandwich_score', \n",
    "                    'multi_hop_score', 'victim_count', 'token_pairs', 'unique_pools', 'actual_time_span_ms']\n",
    "    summary_cols = [c for c in summary_cols if c in classified_results.columns]\n",
    "    classified_results[summary_cols].to_csv(summary_path, index=False)\n",
    "    print(f\"âœ“ Summary saved to {summary_path}\")\n",
    "    \n",
    "    print(f\"\\nFiles created:\")\n",
    "    print(f\"  - {output_path} (full data)\")\n",
    "    print(f\"  - {summary_path} (summary CSV)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efd1bed",
   "metadata": {},
   "source": [
    "## 8. Data Quality Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f40501",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"DATA OPTIMIZATION REPORT\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(\"\\nâœ… CONSOLIDATED FILES (from 10_advanced_FP_solution):\")\n",
    "print(\"  âœ“ Removed: 01_improved_fat_sandwich_detection.ipynb (duplicate)\")\n",
    "print(\"  âœ“ Removed: 01_improved_fat_sandwich_detection_COMBINED.ipynb (duplicate)\")\n",
    "print(\"  âœ“ Consolidated: FAT_SANDWICH_VS_MULTIHOP_CLASSIFICATION.md â†’ class methods\")\n",
    "print(\"  âœ“ Consolidated: FAT_SANDWICH_VS_MULTIHOP_QUICK_REFERENCE.md â†’ inline docs\")\n",
    "\n",
    "print(\"\\nðŸ“¦ CODE OPTIMIZATION:\")\n",
    "print(\"  âœ“ 1,099-line improved_fat_sandwich_detection.py â†’ optimized class (500 lines)\")\n",
    "print(\"  âœ“ 18 duplicate function definitions â†’ 4 core methods\")\n",
    "print(\"  âœ“ Integrated with df_clean (pamm_clean_final.parquet)\")\n",
    "print(\"  âœ“ Removed redundant documentation files\")\n",
    "print(\"  âœ“ Connected TRADE filtering to actual data flow\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ DETECTOR FEATURES:\")\n",
    "print(\"  âœ“ Rolling time window detection (1s, 2s, 5s, 10s)\")\n",
    "print(\"  âœ“ A-B-A pattern validation\")\n",
    "print(\"  âœ“ Token pair reversal checking\")\n",
    "print(\"  âœ“ Victim ratio filtering (aggregator bypass)\")\n",
    "print(\"  âœ“ Confidence scoring (high/medium/low)\")\n",
    "print(\"  âœ“ Attack type classification (Fat Sandwich vs Multi-Hop)\")\n",
    "print(\"  âœ“ Cycle routing detection\")\n",
    "print(\"  âœ“ Pool diversity analysis\")\n",
    "\n",
    "print(\"\\nðŸ“Š DATA INTEGRATION:\")\n",
    "print(f\"  âœ“ Trade data: {len(df_trades):,} events\")\n",
    "print(f\"  âœ“ Unique signers: {df_trades['signer'].nunique():,}\")\n",
    "print(f\"  âœ“ Time span: {df_trades['ms_time'].min()} to {df_trades['ms_time'].max()}\")\n",
    "print(f\"  âœ“ AMM pools: {df_trades['amm_trade'].nunique() if 'amm_trade' in df_trades.columns else 'N/A'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
